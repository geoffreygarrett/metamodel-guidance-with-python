{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ggarrett/lab/stage\n"
     ]
    }
   ],
   "source": [
    "%cd ~/lab/stage\n",
    "from src.dataset import *\n",
    "from src.problems.two_dimensional.representation import NOmegaPointsScaleBasedPeriodic\n",
    "\n",
    "nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "\n",
    "\n",
    "def convert_rotational_eom_to_solution_series(ds, init_theta, n_t_samples, reference_time, save_parquet=True, which={'omega'}):\n",
    "    mapping = NOmegaPointsScaleBasedPeriodic(n_samples=n_t_samples, n_design_points=3, init_theta=init_theta)\n",
    "    \n",
    "    df = ds.get_df()\n",
    "    x_col = list(filter(lambda x: 'x' in x, df.columns))\n",
    "    f_col = list(filter(lambda x: 'f' in x, df.columns))\n",
    "\n",
    "    x_df = ds.get_df().loc[:,x_col]\n",
    "    f_df = ds.get_df().loc[:,f_col]\n",
    "    \n",
    "    idx_f_nan = list(nans(f_df).index)\n",
    "\n",
    "    n_t_samples = n_t_samples\n",
    "    n_samples = x_df.index.size\n",
    "    mapping.reference_time = reference_time\n",
    "\n",
    "    alpha_samples = np.zeros((n_samples,n_t_samples))\n",
    "    omega_samples = np.zeros((n_samples,n_t_samples))\n",
    "    theta_samples = np.zeros((n_samples,n_t_samples))\n",
    "\n",
    "    for idx, row in x_df.iterrows():\n",
    "        print(idx) if idx%100000==0 else None\n",
    "        \n",
    "        alpha_sample, omega_sample, theta_sample = mapping(row.values)\n",
    "        \n",
    "        if 'alpha' in which:\n",
    "            alpha_samples[idx,:] = alpha_sample\n",
    "        if 'omega' in which:\n",
    "            omega_samples[idx,:] = omega_sample\n",
    "        if 'theta' in which:\n",
    "            theta_samples[idx,:] = theta_sample\n",
    "\n",
    "    time_column = [f\"t{i}\" for i in range(n_t_samples)]\n",
    "    dataframe_alpha = pd.DataFrame(data=alpha_samples, columns=time_column)\n",
    "    dataframe_omega = pd.DataFrame(data=omega_samples, columns=time_column)\n",
    "    dataframe_theta = pd.DataFrame(data=theta_samples, columns=time_column)\n",
    "    \n",
    "    dataframe_alpha = dataframe_alpha.drop(idx_f_nan)\n",
    "    dataframe_omega = dataframe_omega.drop(idx_f_nan)\n",
    "    dataframe_theta = dataframe_theta.drop(idx_f_nan)\n",
    "\n",
    "    if 'alpha' in which:\n",
    "        ds_alpha = DataSetFX(output=f_df.values, input=dataframe_alpha.values, name=ds.name+\"_series_alpha\", root_dir=\"data\")\n",
    "    if 'omega' in which:\n",
    "        ds_omega = DataSetFX(output=f_df.values, input=dataframe_omega.values, name=ds.name+\"_series_omega\", root_dir=\"data\")\n",
    "    if 'theta' in which:\n",
    "        ds_theta = DataSetFX(output=f_df.values, input=dataframe_theta.values, name=ds.name+\"_series_theta\", root_dir=\"data\")\n",
    "        \n",
    "    _return = []\n",
    "    if save_parquet:\n",
    "        if 'alpha' in which:\n",
    "            ds_alpha.to_parquet(file_name=ds.name+\"_series_alpha\", root_dir=\"data\")\n",
    "            _return.append(ds_alpha)\n",
    "        if 'omega' in which:\n",
    "            ds_omega.to_parquet(file_name=ds.name+\"_series_omega\", root_dir=\"data\")\n",
    "            _return.append(ds_omega)\n",
    "        if 'theta' in which:\n",
    "            ds_theta.to_parquet(file_name=ds.name+\"_series_theta\", root_dir=\"data\")\n",
    "            _return.append(ds_theta)\n",
    "        \n",
    "    return _return\n",
    "\n",
    "\n",
    "nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "\n",
    "def convert_rotational_eom_to_omega_series(ds, init_theta, n_t_samples, reference_time, save_parquet=True):\n",
    "    mapping = NOmegaPointsScaleBasedPeriodic(n_samples=n_t_samples, n_design_points=3, init_theta=init_theta)\n",
    "    df = ds.get_df().dropna().reset_index(drop=True)\n",
    "    \n",
    "    x_col = list(filter(lambda x: 'x' in x, df.columns))\n",
    "    f_col = list(filter(lambda x: 'f' in x, df.columns))\n",
    "\n",
    "    x_df = df.loc[:,x_col]\n",
    "    f_df = df.loc[:,f_col]\n",
    "    \n",
    "    n_t_samples = n_t_samples\n",
    "    n_samples = x_df.index.size\n",
    "    mapping.reference_time = reference_time\n",
    "\n",
    "    omega_samples = np.zeros((n_samples,n_t_samples))\n",
    "\n",
    "    for idx, row in x_df.iterrows():\n",
    "        print(idx) if idx%100000==0 else None\n",
    "        alpha_sample, omega_sample, theta_sample = mapping(row.values)\n",
    "        omega_samples[idx,:] = omega_sample\n",
    "\n",
    "    time_column = [f\"t{i}\" for i in range(n_t_samples)]\n",
    "    dataframe_omega = pd.DataFrame(data=omega_samples, columns=time_column)\n",
    "    ds_omega = DataSetFX(output=f_df.values, input=dataframe_omega.values, name=ds.name+\"_series_omega\", root_dir=\"data\")\n",
    "\n",
    "    _return = []\n",
    "    if save_parquet:\n",
    "\n",
    "        ds_omega.to_parquet(file_name=ds.name+\"_series_omega\", root_dir=\"data\")\n",
    "        \n",
    "    return ds_omega\n",
    "\n",
    "#https://open.spotify.com/track/1ibeKVCiXORhvUpMmtsQWq?si=fVJAVtOtTHmZm0KPglam-w\n",
    "\n",
    "#https://open.spotify.com/track/1ibeKVCiXORhvUpMmtsQWq?si=fVJAVtOtTHmZm0KPglam-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "ds = DataSetFX.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5\")\n",
    "\n",
    "# df = ds.get_df()\n",
    "# df = df.sample(frac=0.5)\n",
    "# ds = DataSetFX(input=df.loc[:,filter(lambda x: 'x' in x, df.columns)].values,\n",
    "#                output=df.loc[:,filter(lambda x: 'f' in x, df.columns)].values,\n",
    "#                root_dir=\"data/AttitudeTrajectoryProblem2D_C1_R1\", file_name=\"dataSet2_haltonSampled_6_sdp5.parquet\")\n",
    "\n",
    "# ds.to_parquet(file_name=ds.name, root_dir='data')\n",
    "\n",
    "reference_time = 6529.276071694349\n",
    "n_t_samples = 652\n",
    "init_theta = -3.141592653589793\n",
    "\n",
    "ds_omega = convert_rotational_eom_to_omega_series(ds, init_theta, n_t_samples, reference_time)\n",
    "\n",
    "# nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "# nans(ds_omega.get_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args:\n",
    "- in_channels (int): Number of channels in the input image\n",
    "- out_channels (int): Number of channels produced by the convolution\n",
    "- kernel_size (int or tuple): Size of the convolving kernel\n",
    "- stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "- padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "- padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
    "- dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "- groups (int, optional): Number of blocked connections from input channels to output channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.get_df())\n",
    "print(ds.get_df().dropna().reset_index(drop=True))\n",
    "\n",
    "from src.surrogate.modules import *\n",
    "from src.surrogate.deeplearning.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case 1\n",
    "\n",
    "transform\n",
    "\n",
    "Data:\n",
    "- F = F, X = Theta\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "\n",
    "tf1_1 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_1 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_2 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_2 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_3 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_3 = preprocessing.StandardScaler()\n",
    "\n",
    "transform_1 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_1, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_1, [f\"x{i}\" for i in range(1,653)]))\n",
    "#     (\"tx1\", (tx2, [f\"x{i}\" for i in range(652)])),\n",
    "]\n",
    "))\n",
    "\n",
    "transform_2 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_2, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_2, [f\"x{i}\" for i in range(1,653)]))\n",
    "#     (\"tx1\", (tx2, [f\"x{i}\" for i in range(652)])),\n",
    "]\n",
    "))\n",
    "transform_3 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_3, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_3, [f\"x{i}\" for i in range(1,653)]))\n",
    "#     (\"tx1\", (tx2, [f\"x{i}\" for i in range(652)])),\n",
    "]\n",
    "))\n",
    "# NB10422645 (Stage - Airbus)/data/\n",
    "ds_t1_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_randomSampled_6_series_omega\", transform=transform_1)\n",
    "ds_t2_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_series_omega\", transform=transform_2)\n",
    "ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_3)\n",
    "\n",
    "# Define training function.\n",
    "def train_model(name, ds, model, model_folder, lr=1e-1, epoch=50, load=False, scheduler=None):\n",
    "    trainer = Trainer(model_folder, ds, model, torch.optim.Adam, torch.nn.MSELoss, num_epochs=epoch, batch_size=int(1e2), learning_rate=lr)\n",
    "    if scheduler is not None:\n",
    "        trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=scheduler[0], gamma=scheduler[1])\n",
    "    if load:\n",
    "        trainer.load_checkpoint(model_folder)\n",
    "    trainer.train()\n",
    "    return trainer._model\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "name = \"experiment1_series\"\n",
    "folder = f\"models/{name}\"\n",
    "\n",
    "# ds = ds_t1_omega\n",
    "# print(ds.get_df())\n",
    "\n",
    "# model1 = nn.Sequential(\n",
    "#     BasicBlock(input_size=1, output_size=50, kernel_size=(1, 10), stride=2),\n",
    "#     torch.nn.MaxPool2d((1, 50), stride=1, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n",
    "# #     ResidualBlock(hidden_size=50, kernel_size=(1,10), stride=1),\n",
    "# #     BasicBlock(input_size=20, output_size=10, kernel_size=(1, 20), stride=1),\n",
    "# #     torch.nn.MaxPool2d((1, 10), stride=1, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n",
    "# #     BasicBlock(input_size=10, output_size=5, kernel_size=(1, 10), stride=1),\n",
    "    \n",
    "#     Flatten(),\n",
    "#     nn.Conv2d(500, 50, 1),\n",
    "#     activation_dict[\"leaky_relu\"],\n",
    "#     RegressionOutput(50, 4),\n",
    "    \n",
    "# )\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "    \n",
    "    nn.Conv2d(1, 64, stride=4, kernel_size=(1,36), padding=0),   # [64,   1, 155]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool2d((1,11), stride=2, padding=(0,2)),               # [64,   1,  73]\n",
    "    \n",
    "    \n",
    "    nn.Conv2d(64, 128, stride=1, kernel_size=(1,19), padding=9), # [128,  1,  73]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool2d((1,4), stride=3, padding=(0,2)),                # [128,  1,  24]\n",
    "    \n",
    "    nn.Conv2d(128,172, stride=1, kernel_size=(1,3), padding=1),  # [172,  1,  24]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool2d((1,4), stride=2, padding=(0,2)),                # [172,  1,  11]\n",
    "    \n",
    "    nn.Conv2d(172,128, stride=1, kernel_size=(1,3), padding=1),  # [128,  1,  11]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool2d((1,4), stride=2, padding=(0,2)),                # [128,  1,   5]\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(4096, 4096),                                         # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    \n",
    "    nn.Linear(4096, 4096),                                         # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    \n",
    "    nn.Linear(4096, 4),\n",
    "    Unflatten()\n",
    "    )\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)\n",
    "\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "    \n",
    "    Flatten2(),\n",
    "    \n",
    "    nn.Conv1d(1, 126, stride=4, kernel_size=72, padding=0),       # [64,   1, 155]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool1d(11, stride=2, padding=0),                       # [64,   1,  73]\n",
    "    \n",
    "    nn.Conv1d(126, 258, stride=1, kernel_size=19, padding=9),     # [128,  1,  73]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool1d(4, stride=3, padding=0),                        # [128,  1,  24]\n",
    "    \n",
    "    nn.Conv1d(258, 258, stride=1, kernel_size=3, padding=1),     # [172,  1,  24]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [172,  1,  11]\n",
    "    \n",
    "    nn.Conv1d(258,126, stride=1, kernel_size=3, padding=1),      # [128,  1,  11]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [128,  1,   5]\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    \n",
    "    nn.Linear(504, 5),\n",
    "    Unflatten()\n",
    "    )\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\"dataSet2_randomSampled_6_\", ds_t1_omega, model2, folder, load=True,\n",
    "           scheduler=([50], 0.1),\n",
    "           epoch=100,\n",
    "           lr=1e-4)\n",
    "# https://open.spotify.com/track/0rUNZQuYQvOz6A6zwyT6tM?si=6WSixNxgSfW2FTLLytNB8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\"dataSet2_haltonSampled_6_\", ds_t2_omega, model2, folder, load=False,\n",
    "           scheduler=([50], 0.1),\n",
    "           epoch=100,\n",
    "           lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\"dataSet2_gridSampled_6_\", ds_t2_omega, model2, folder, load=False,\n",
    "           scheduler=([50], 0.1),\n",
    "           epoch=100,\n",
    "           lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = list(model.children())[1].weight[0]\n",
    "print(len(x_.flatten()))\n",
    "print(list(model.children())[1].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "count = 0\n",
    "for res in list(model.children())[1].weight[::1]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.children())[4].weight.shape)\n",
    "\n",
    "\n",
    "# for res in list(model.children())[4].weight[::2]:\n",
    "# # res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "#     plt.figure(dpi=400, figsize=(10,2))\n",
    "#     plt.axis('off')\n",
    "#     imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(20,1)), cmap='gray')\n",
    "\n",
    "res = list(model.children())[14].weight.detach().cpu().numpy()[:,:,]\n",
    "\n",
    "# avg = np.mean(res, axis=2)\n",
    "print(res.shape)\n",
    "\n",
    "plt.figure(dpi=500, figsize=(10,2))\n",
    "plt.axis('off')\n",
    "# imgplot = plt.imshow(avg[np.mean(avg,axis=1).argsort()], cmap='gray')\n",
    "imgplot = plt.imshow(res, cmap='gray')\n",
    "\n",
    "\n",
    "print(avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for res in avg:\n",
    "# res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in list(model.children())[10].weight[::3]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
