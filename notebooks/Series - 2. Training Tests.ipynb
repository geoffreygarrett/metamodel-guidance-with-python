{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args:\n",
    "- in_channels (int): Number of channels in the input image\n",
    "- out_channels (int): Number of channels produced by the convolution\n",
    "- kernel_size (int or tuple): Size of the convolving kernel\n",
    "- stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "- padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "- padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
    "- dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "- groups (int, optional): Number of blocked connections from input channels to output channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case 1\n",
    "\n",
    "transform\n",
    "\n",
    "Data:\n",
    "- F = F, X = Theta\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ggarrett/lab/stage\n"
     ]
    }
   ],
   "source": [
    "%cd ~/lab/stage/             \n",
    "from src.surrogate.modules import *\n",
    "from src.surrogate.deeplearning.core import *\n",
    "from src.dataset import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "\n",
    "tf1_1 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_1 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_2 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_2 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_3 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_3 = preprocessing.StandardScaler()\n",
    "\n",
    "transform_1 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_1, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_1, [f\"x{i}\" for i in range(1,653)]))\n",
    "]\n",
    "))\n",
    "\n",
    "transform_2 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_2, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_2, [f\"x{i}\" for i in range(1,301)]))\n",
    "]\n",
    "))\n",
    "transform_3 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_3, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_3, [f\"x{i}\" for i in range(1,653)]))\n",
    "]\n",
    "))\n",
    "\n",
    "# NB10422645 (Stage - Airbus)/data/\n",
    "# ds_t1_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_1)\n",
    "# ds_t2_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_randomSampled_6_series_omega\", transform=transform_2)\n",
    "# ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5_series_omega\", transform=transform_3)\n",
    "\n",
    "nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "\n",
    "# nans(ds_t2_omega.get_df())\n",
    "\n",
    "# ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function.\n",
    "def train_model(name, ds, model, model_folder, lr=1e-1, epoch=50, load=False, scheduler=None):\n",
    "    trainer = Trainer(model_folder, ds, model, torch.optim.Adam, torch.nn.MSELoss, num_epochs=epoch, batch_size=int(1e2), learning_rate=lr)\n",
    "    if scheduler is not None:\n",
    "        trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=scheduler[0], gamma=scheduler[1])\n",
    "    if load:\n",
    "        trainer.load_checkpoint(model_folder)\n",
    "    trainer.train()\n",
    "    return trainer._model\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Sequential(\n",
      "  (0): Flatten2()\n",
      "  (1): Conv1d(1, 126, kernel_size=(72,), stride=(4,))\n",
      "  (2): Sigmoid()\n",
      "  (3): MaxPool1d(kernel_size=11, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv1d(126, 258, kernel_size=(19,), stride=(1,), padding=(9,))\n",
      "  (5): Sigmoid()\n",
      "  (6): MaxPool1d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv1d(258, 258, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (8): Sigmoid()\n",
      "  (9): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv1d(258, 126, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (11): Sigmoid()\n",
      "  (12): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): Flatten()\n",
      "  (14): Linear(in_features=504, out_features=504, bias=True)\n",
      "  (15): Sigmoid()\n",
      "  (16): Linear(in_features=504, out_features=504, bias=True)\n",
      "  (17): Sigmoid()\n",
      "  (18): Linear(in_features=504, out_features=5, bias=True)\n",
      "  (19): Unflatten()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)\n",
    "\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "    \n",
    "    Flatten2(),\n",
    "    \n",
    "    nn.Conv1d(1, 126, stride=4, kernel_size=72, padding=0),       # [64,   1, 155]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "    nn.MaxPool1d(11, stride=2, padding=0),                       # [64,   1,  73]\n",
    "#     nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(126, 258, stride=1, kernel_size=19, padding=9),     # [128,  1,  73]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=3, padding=0),                        # [128,  1,  24]\n",
    "#     nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(258, 258, stride=1, kernel_size=3, padding=1),     # [172,  1,  24]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [172,  1,  11]\n",
    "#     nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(258,126, stride=1, kernel_size=3, padding=1),      # [128,  1,  11]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [128,  1,   5]\n",
    "#     nn.Dropout(0.05),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "\n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "#     nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "    nn.Sigmoid(),             # \n",
    "\n",
    "    \n",
    "    nn.Linear(504, 5),\n",
    "    Unflatten()\n",
    "    )\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name = \"dataSet2_gridSampled_6_model\"\n",
    "# folder = f\"models/{name}\"\n",
    "\n",
    "# ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_1)\n",
    "# model_grid = train_model(\"dataSet2_gridSampled_6_model\", ds, model2, folder, load=False,\n",
    "# #                            scheduler=([10], 0.1),\n",
    "#                          epoch=50,\n",
    "#                          lr=1e-4)\n",
    "\n",
    "# # # https://open.spotify.com/track/0rUNZQuYQvOz6A6zwyT6tM?si=6WSixNxgSfW2FTLLytNB8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1  / 50  ]  |  Train Loss:  0.009242  |  Test Loss:  0.002754  |  lr:  0.00010\n",
      "Epoch [  2  / 50  ]  |  Train Loss:  0.002901  |  Test Loss:  0.002828  |  lr:  0.00010\n",
      "Epoch [  3  / 50  ]  |  Train Loss:  0.002907  |  Test Loss:  0.002724  |  lr:  0.00010\n",
      "Epoch [  4  / 50  ]  |  Train Loss:  0.002917  |  Test Loss:  0.002977  |  lr:  0.00010\n",
      "Epoch [  5  / 50  ]  |  Train Loss:  0.002916  |  Test Loss:  0.002808  |  lr:  0.00010\n",
      "Epoch [  6  / 50  ]  |  Train Loss:  0.002922  |  Test Loss:  0.002928  |  lr:  0.00010\n",
      "Epoch [  7  / 50  ]  |  Train Loss:  0.002934  |  Test Loss:  0.002797  |  lr:  0.00010\n",
      "Epoch [  8  / 50  ]  |  Train Loss:  0.002924  |  Test Loss:  0.002767  |  lr:  0.00010\n",
      "Epoch [  9  / 50  ]  |  Train Loss:  0.002925  |  Test Loss:  0.002984  |  lr:  0.00010\n",
      "Epoch [ 10  / 50  ]  |  Train Loss:  0.002905  |  Test Loss:  0.002762  |  lr:  0.00010\n",
      "Epoch [ 11  / 50  ]  |  Train Loss:  0.002887  |  Test Loss:  0.002738  |  lr:  0.00010\n",
      "Epoch [ 12  / 50  ]  |  Train Loss:  0.002837  |  Test Loss:  0.002660  |  lr:  0.00010\n",
      "Epoch [ 13  / 50  ]  |  Train Loss:  0.002779  |  Test Loss:  0.002545  |  lr:  0.00010\n",
      "Epoch [ 14  / 50  ]  |  Train Loss:  0.002706  |  Test Loss:  0.002454  |  lr:  0.00010\n",
      "Epoch [ 15  / 50  ]  |  Train Loss:  0.002649  |  Test Loss:  0.002543  |  lr:  0.00010\n",
      "Epoch [ 16  / 50  ]  |  Train Loss:  0.002605  |  Test Loss:  0.002497  |  lr:  0.00010\n",
      "Epoch [ 17  / 50  ]  |  Train Loss:  0.002546  |  Test Loss:  0.002403  |  lr:  0.00010\n",
      "Epoch [ 18  / 50  ]  |  Train Loss:  0.002527  |  Test Loss:  0.002286  |  lr:  0.00010\n",
      "Epoch [ 19  / 50  ]  |  Train Loss:  0.002506  |  Test Loss:  0.002304  |  lr:  0.00010\n",
      "Epoch [ 20  / 50  ]  |  Train Loss:  0.002488  |  Test Loss:  0.002658  |  lr:  0.00010\n",
      "Epoch [ 21  / 50  ]  |  Train Loss:  0.002467  |  Test Loss:  0.002471  |  lr:  0.00010\n",
      "Epoch [ 22  / 50  ]  |  Train Loss:  0.002447  |  Test Loss:  0.002296  |  lr:  0.00010\n",
      "Epoch [ 23  / 50  ]  |  Train Loss:  0.002437  |  Test Loss:  0.002248  |  lr:  0.00010\n",
      "Epoch [ 24  / 50  ]  |  Train Loss:  0.002415  |  Test Loss:  0.002332  |  lr:  0.00010\n",
      "Epoch [ 25  / 50  ]  |  Train Loss:  0.002405  |  Test Loss:  0.002231  |  lr:  0.00010\n",
      "Epoch [ 26  / 50  ]  |  Train Loss:  0.002393  |  Test Loss:  0.002441  |  lr:  0.00010\n",
      "Epoch [ 27  / 50  ]  |  Train Loss:  0.002378  |  Test Loss:  0.002316  |  lr:  0.00010\n",
      "Epoch [ 28  / 50  ]  |  Train Loss:  0.002360  |  Test Loss:  0.002203  |  lr:  0.00010\n",
      "Epoch [ 29  / 50  ]  |  Train Loss:  0.002343  |  Test Loss:  0.002197  |  lr:  0.00010\n",
      "Epoch [ 30  / 50  ]  |  Train Loss:  0.002334  |  Test Loss:  0.002127  |  lr:  0.00010\n",
      "Epoch [ 31  / 50  ]  |  Train Loss:  0.002318  |  Test Loss:  0.002394  |  lr:  0.00010\n",
      "Epoch [ 32  / 50  ]  |  Train Loss:  0.002299  |  Test Loss:  0.002130  |  lr:  0.00010\n",
      "Epoch [ 33  / 50  ]  |  Train Loss:  0.002299  |  Test Loss:  0.002165  |  lr:  0.00010\n",
      "Epoch [ 34  / 50  ]  |  Train Loss:  0.002272  |  Test Loss:  0.002203  |  lr:  0.00010\n",
      "Epoch [ 35  / 50  ]  |  Train Loss:  0.002265  |  Test Loss:  0.002149  |  lr:  0.00010\n",
      "Epoch [ 36  / 50  ]  |  Train Loss:  0.002258  |  Test Loss:  0.002177  |  lr:  0.00010\n",
      "Epoch [ 37  / 50  ]  |  Train Loss:  0.002240  |  Test Loss:  0.002100  |  lr:  0.00010\n",
      "Epoch [ 38  / 50  ]  |  Train Loss:  0.002232  |  Test Loss:  0.002189  |  lr:  0.00010\n",
      "Epoch [ 39  / 50  ]  |  Train Loss:  0.002210  |  Test Loss:  0.002105  |  lr:  0.00010\n",
      "Epoch [ 40  / 50  ]  |  Train Loss:  0.002241  |  Test Loss:  0.002190  |  lr:  0.00010\n",
      "Epoch [ 41  / 50  ]  |  Train Loss:  0.002196  |  Test Loss:  0.002077  |  lr:  0.00010\n",
      "Epoch [ 42  / 50  ]  |  Train Loss:  0.002187  |  Test Loss:  0.002075  |  lr:  0.00010\n",
      "Epoch [ 43  / 50  ]  |  Train Loss:  0.002179  |  Test Loss:  0.002049  |  lr:  0.00010\n",
      "Epoch [ 44  / 50  ]  |  Train Loss:  0.002176  |  Test Loss:  0.002061  |  lr:  0.00010\n",
      "Epoch [ 45  / 50  ]  |  Train Loss:  0.002154  |  Test Loss:  0.002087  |  lr:  0.00010\n",
      "Epoch [ 46  / 50  ]  |  Train Loss:  0.002155  |  Test Loss:  0.002020  |  lr:  0.00010\n",
      "Epoch [ 47  / 50  ]  |  Train Loss:  0.002139  |  Test Loss:  0.002043  |  lr:  0.00010\n",
      "Epoch [ 48  / 50  ]  |  Train Loss:  0.002147  |  Test Loss:  0.002045  |  lr:  0.00010\n",
      "Epoch [ 49  / 50  ]  |  Train Loss:  0.002135  |  Test Loss:  0.001984  |  lr:  0.00010\n",
      "Epoch [ 50  / 50  ]  |  Train Loss:  0.002128  |  Test Loss:  0.002161  |  lr:  0.00010\n"
     ]
    }
   ],
   "source": [
    "name = \"dataSet2_randomSampled_6_model\"\n",
    "folder = f\"models/{name}\"\n",
    "\n",
    "ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_randomSampled_6_series_omega\", transform=transform_2)\n",
    "model_random = train_model(\"dataSet2_randomSampled_6_model\", ds, model2, folder, load=False,\n",
    "#                            scheduler=([10], 0.1),\n",
    "                         epoch=50,\n",
    "                         lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"dataSet2_haltonSampled_6_model\"\n",
    "# folder = f\"models/{name}\"\n",
    "\n",
    "# ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5_series_omega\", transform=transform_3)\n",
    "# model_halton = train_model(\"dataSet2_haltonSampled_6_model\", ds, model2, folder, load=False,\n",
    "# #                            scheduler=([10], 0.1),\n",
    "#                          epoch=50,\n",
    "#                          lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1  / 50  ]  |  Train Loss:  0.011992  |  Test Loss:  0.002763  |  lr:  0.00010\n",
      "Epoch [  2  / 50  ]  |  Train Loss:  0.001873  |  Test Loss:  0.001279  |  lr:  0.00010\n",
      "Epoch [  3  / 50  ]  |  Train Loss:  0.001010  |  Test Loss:  0.000865  |  lr:  0.00010\n",
      "Epoch [  4  / 50  ]  |  Train Loss:  0.000703  |  Test Loss:  0.000601  |  lr:  0.00010\n",
      "Epoch [  5  / 50  ]  |  Train Loss:  0.000542  |  Test Loss:  0.000467  |  lr:  0.00010\n",
      "Epoch [  6  / 50  ]  |  Train Loss:  0.000441  |  Test Loss:  0.000445  |  lr:  0.00010\n",
      "Epoch [  7  / 50  ]  |  Train Loss:  0.000367  |  Test Loss:  0.000343  |  lr:  0.00010\n",
      "Epoch [  8  / 50  ]  |  Train Loss:  0.000313  |  Test Loss:  0.000307  |  lr:  0.00010\n",
      "Epoch [  9  / 50  ]  |  Train Loss:  0.000274  |  Test Loss:  0.000275  |  lr:  0.00010\n",
      "Epoch [ 10  / 50  ]  |  Train Loss:  0.000248  |  Test Loss:  0.000236  |  lr:  0.00010\n",
      "Epoch [ 11  / 50  ]  |  Train Loss:  0.000196  |  Test Loss:  0.000203  |  lr:  0.00001\n",
      "Epoch [ 12  / 50  ]  |  Train Loss:  0.000193  |  Test Loss:  0.000204  |  lr:  0.00001\n",
      "Epoch [ 13  / 50  ]  |  Train Loss:  0.000190  |  Test Loss:  0.000201  |  lr:  0.00001\n",
      "Epoch [ 14  / 50  ]  |  Train Loss:  0.000187  |  Test Loss:  0.000200  |  lr:  0.00001\n",
      "Epoch [ 15  / 50  ]  |  Train Loss:  0.000183  |  Test Loss:  0.000185  |  lr:  0.00001\n",
      "Epoch [ 16  / 50  ]  |  Train Loss:  0.000179  |  Test Loss:  0.000180  |  lr:  0.00001\n",
      "Epoch [ 17  / 50  ]  |  Train Loss:  0.000175  |  Test Loss:  0.000178  |  lr:  0.00001\n",
      "Epoch [ 18  / 50  ]  |  Train Loss:  0.000171  |  Test Loss:  0.000173  |  lr:  0.00001\n",
      "Epoch [ 19  / 50  ]  |  Train Loss:  0.000167  |  Test Loss:  0.000177  |  lr:  0.00001\n",
      "Epoch [ 20  / 50  ]  |  Train Loss:  0.000163  |  Test Loss:  0.000172  |  lr:  0.00001\n",
      "Epoch [ 21  / 50  ]  |  Train Loss:  0.000160  |  Test Loss:  0.000162  |  lr:  0.00001\n",
      "Epoch [ 22  / 50  ]  |  Train Loss:  0.000157  |  Test Loss:  0.000160  |  lr:  0.00001\n",
      "Epoch [ 23  / 50  ]  |  Train Loss:  0.000153  |  Test Loss:  0.000156  |  lr:  0.00001\n",
      "Epoch [ 24  / 50  ]  |  Train Loss:  0.000150  |  Test Loss:  0.000155  |  lr:  0.00001\n",
      "Epoch [ 25  / 50  ]  |  Train Loss:  0.000146  |  Test Loss:  0.000153  |  lr:  0.00001\n",
      "Epoch [ 26  / 50  ]  |  Train Loss:  0.000143  |  Test Loss:  0.000149  |  lr:  0.00001\n",
      "Epoch [ 27  / 50  ]  |  Train Loss:  0.000140  |  Test Loss:  0.000146  |  lr:  0.00001\n",
      "Epoch [ 28  / 50  ]  |  Train Loss:  0.000137  |  Test Loss:  0.000144  |  lr:  0.00001\n",
      "Epoch [ 29  / 50  ]  |  Train Loss:  0.000134  |  Test Loss:  0.000141  |  lr:  0.00001\n",
      "Epoch [ 30  / 50  ]  |  Train Loss:  0.000132  |  Test Loss:  0.000139  |  lr:  0.00001\n",
      "Epoch [ 31  / 50  ]  |  Train Loss:  0.000129  |  Test Loss:  0.000135  |  lr:  0.00001\n",
      "Epoch [ 32  / 50  ]  |  Train Loss:  0.000126  |  Test Loss:  0.000133  |  lr:  0.00001\n",
      "Epoch [ 33  / 50  ]  |  Train Loss:  0.000124  |  Test Loss:  0.000126  |  lr:  0.00001\n",
      "Epoch [ 34  / 50  ]  |  Train Loss:  0.000121  |  Test Loss:  0.000135  |  lr:  0.00001\n",
      "Epoch [ 35  / 50  ]  |  Train Loss:  0.000119  |  Test Loss:  0.000124  |  lr:  0.00001\n",
      "Epoch [ 36  / 50  ]  |  Train Loss:  0.000117  |  Test Loss:  0.000126  |  lr:  0.00001\n",
      "Epoch [ 37  / 50  ]  |  Train Loss:  0.000115  |  Test Loss:  0.000124  |  lr:  0.00001\n"
     ]
    }
   ],
   "source": [
    "name = \"dataSet2_sol1_gridSampled_6_model\"\n",
    "folder = f\"models/{name}\"\n",
    "\n",
    "ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_sol1_gridSampled_6_series_omega\", transform=transform_3)\n",
    "model_sol1 = train_model(\"vdataSet2_sol1_gridSampled_6_model\", ds, model2, folder, load=False,\n",
    "                           scheduler=([10], 0.1),\n",
    "                         epoch=50,\n",
    "                         lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = list(model_halton.children())[1].weight[0]\n",
    "print(len(x_.flatten()))\n",
    "print(list(model_halton.children())[1].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_halton.eval()\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "count = 0\n",
    "for res in list(model_halton.children())[1].weight[::1]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_halton\n",
    "\n",
    "\n",
    "# print(list(model.children())[4].weight.shape)\n",
    "\n",
    "\n",
    "# for res in list(model.children())[4].weight[::2]:\n",
    "# # res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "#     plt.figure(dpi=400, figsize=(10,2))\n",
    "#     plt.axis('off')\n",
    "#     imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(20,1)), cmap='gray')\n",
    "\n",
    "res = list(model.children())[13].weight.detach().cpu().numpy()[:,:,]\n",
    "\n",
    "avg = np.mean(res, axis=1)\n",
    "print(res.shape)\n",
    "\n",
    "plt.figure(dpi=500, figsize=(10,2))\n",
    "plt.axis('off')\n",
    "imgplot = plt.imshow(avg, cmap='gray')\n",
    "# imgplot = plt.imshow(res, cmap='gray')\n",
    "\n",
    "\n",
    "# print(avg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for res in avg:\n",
    "# res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in list(model.children())[9].weight[::3]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
