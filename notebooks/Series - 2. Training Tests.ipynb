{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "                   \n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args:\n",
    "- in_channels (int): Number of channels in the input image\n",
    "- out_channels (int): Number of channels produced by the convolution\n",
    "- kernel_size (int or tuple): Size of the convolving kernel\n",
    "- stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "- padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "- padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\n",
    "- dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "- groups (int, optional): Number of blocked connections from input channels to output channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case 1\n",
    "\n",
    "transform\n",
    "\n",
    "Data:\n",
    "- F = F, X = Theta\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ggarrett/lab/NB10422645\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Passed non-file path: data/dataSet2_fftSampled_16_3_3e5_series_omega",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-73494088dcfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# NB10422645 (Stage - Airbus)/data/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mds_t1_omega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSetFSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataSet2_fftSampled_16_3_3e5_series_omega\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# ds_t2_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_randomSampled_6_series_omega\", transform=transform_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5_series_omega\", transform=transform_3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/NB10422645/src/dataset/_core.py\u001b[0m in \u001b[0;36mfrom_parquet\u001b[0;34m(cls, file_name, root_dir, transform)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0m_fx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_to_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sigh/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sigh/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_pandas_metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         result = self.api.parquet.read_table(\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         ).to_pandas()\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sigh/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, filesystem, filters)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m         pf = ParquetDataset(source, metadata=metadata, memory_map=memory_map,\n\u001b[0;32m-> 1212\u001b[0;31m                             filesystem=filesystem, filters=filters)\n\u001b[0m\u001b[1;32m   1213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sigh/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, memory_map)\u001b[0m\n\u001b[1;32m    979\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_manifest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m              \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m              open_file_func=partial(_open_dataset_file, self))\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_metadata_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sigh/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m_make_manifest\u001b[0;34m(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 raise IOError('Passed non-file path: {0}'\n\u001b[0;32m-> 1167\u001b[0;31m                               .format(path))\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mpiece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetDatasetPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_file_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen_file_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Passed non-file path: data/dataSet2_fftSampled_16_3_3e5_series_omega"
     ]
    }
   ],
   "source": [
    "%cd ~/lab/NB10422645/   \n",
    "from src.surrogate.modules import *\n",
    "from src.surrogate.deeplearning.core import *\n",
    "from src.dataset import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "\n",
    "tf1_1 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_1 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_2 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_2 = preprocessing.StandardScaler()\n",
    "\n",
    "tf1_3 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "tx1_3 = preprocessing.StandardScaler()\n",
    "\n",
    "transform_1 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_1, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_1, [f\"x{i}\" for i in range(1,653)]))\n",
    "]\n",
    "))\n",
    "\n",
    "transform_2 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_2, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_2, [f\"x{i}\" for i in range(1,301)]))\n",
    "]\n",
    "))\n",
    "transform_3 = TransformDataFrame.from_ordered_dict(OrderedDict([\n",
    "    (\"tf1\", (tf1_3, [\"f1\", \"f2\", \"f3\", \"f4\", 'f5'])),\n",
    "    (\"tx1\", (tx1_3, [f\"x{i}\" for i in range(1,653)]))\n",
    "]\n",
    "))\n",
    "\n",
    "# NB10422645 (Stage - Airbus)/data/\n",
    "ds_t1_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_fftSampled_16_3_3e5_series_omega\", transform=transform_1)\n",
    "# ds_t2_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_randomSampled_6_series_omega\", transform=transform_2)\n",
    "# ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5_series_omega\", transform=transform_3)\n",
    "\n",
    "nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "\n",
    "# nans(ds_t2_omega.get_df())\n",
    "\n",
    "# ds_t3_omega = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function.\n",
    "def train_model(name, ds, model, model_folder, lr=1e-1, epoch=50, load=False, scheduler=None):\n",
    "    trainer = Trainer(model_folder, ds, model, torch.optim.Adam, torch.nn.MSELoss, num_epochs=epoch, batch_size=int(1e2), learning_rate=lr)\n",
    "    if scheduler is not None:\n",
    "        trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=scheduler[0], gamma=scheduler[1])\n",
    "    if load:\n",
    "        trainer.load_checkpoint(model_folder)\n",
    "    trainer.train()\n",
    "    return trainer._model\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Sequential(\n",
      "  (0): Flatten2()\n",
      "  (1): Conv1d(1, 126, kernel_size=(72,), stride=(4,))\n",
      "  (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (3): MaxPool1d(kernel_size=11, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Dropout(p=0.05, inplace=False)\n",
      "  (5): Conv1d(126, 258, kernel_size=(19,), stride=(1,), padding=(9,))\n",
      "  (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (7): MaxPool1d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Dropout(p=0.05, inplace=False)\n",
      "  (9): Conv1d(258, 258, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (11): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (12): Dropout(p=0.05, inplace=False)\n",
      "  (13): Conv1d(258, 126, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (14): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (15): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (16): Dropout(p=0.05, inplace=False)\n",
      "  (17): Flatten()\n",
      "  (18): Linear(in_features=504, out_features=504, bias=True)\n",
      "  (19): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (20): Linear(in_features=504, out_features=504, bias=True)\n",
      "  (21): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (22): Linear(in_features=504, out_features=5, bias=True)\n",
      "  (23): Unflatten()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class Flatten2(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), input.size(1), 1, 1)\n",
    "\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "    \n",
    "    Flatten2(),\n",
    "    \n",
    "    nn.Conv1d(1, 126, stride=4, kernel_size=72, padding=0),       # [64,   1, 155]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "    nn.MaxPool1d(11, stride=2, padding=0),                       # [64,   1,  73]\n",
    "    nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(126, 258, stride=1, kernel_size=19, padding=9),     # [128,  1,  73]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=3, padding=0),                        # [128,  1,  24]\n",
    "    nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(258, 258, stride=1, kernel_size=3, padding=1),     # [172,  1,  24]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [172,  1,  11]\n",
    "    nn.Dropout(0.05),\n",
    "    \n",
    "    nn.Conv1d(258,126, stride=1, kernel_size=3, padding=1),      # [128,  1,  11]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "\n",
    "    nn.MaxPool1d(4, stride=2, padding=0),                        # [128,  1,   5]\n",
    "    nn.Dropout(0.05),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "\n",
    "    \n",
    "    nn.Linear(504, 504),                                       # [,860]\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=True),             # \n",
    "#     nn.Sigmoid(),             # \n",
    "\n",
    "    \n",
    "    nn.Linear(504, 5),\n",
    "    Unflatten()\n",
    "    )\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name = \"dataSet2_gridSampled_6_model\"\n",
    "# folder = f\"models/{name}\"\n",
    "\n",
    "# ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_gridSampled_6_series_omega\", transform=transform_1)\n",
    "# model_grid = train_model(\"dataSet2_gridSampled_6_model\", ds, model2, folder, load=False,\n",
    "# #                            scheduler=([10], 0.1),\n",
    "#                          epoch=50,\n",
    "#                          lr=1e-4)\n",
    "\n",
    "# # # https://open.spotify.com/track/0rUNZQuYQvOz6A6zwyT6tM?si=6WSixNxgSfW2FTLLytNB8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1  / 50  ]  |  Train Loss:  0.008052  |  Test Loss:  0.003716  |  lr:  0.00100\n",
      "Epoch [  2  / 50  ]  |  Train Loss:  0.003327  |  Test Loss:  0.003082  |  lr:  0.00100\n",
      "Epoch [  3  / 50  ]  |  Train Loss:  0.002857  |  Test Loss:  0.002947  |  lr:  0.00100\n",
      "Epoch [  4  / 50  ]  |  Train Loss:  0.002703  |  Test Loss:  0.002993  |  lr:  0.00100\n",
      "Epoch [  5  / 50  ]  |  Train Loss:  0.002593  |  Test Loss:  0.002720  |  lr:  0.00100\n",
      "Epoch [  6  / 50  ]  |  Train Loss:  0.002507  |  Test Loss:  0.002601  |  lr:  0.00100\n",
      "Epoch [  7  / 50  ]  |  Train Loss:  0.002445  |  Test Loss:  0.002668  |  lr:  0.00100\n",
      "Epoch [  8  / 50  ]  |  Train Loss:  0.002422  |  Test Loss:  0.002658  |  lr:  0.00100\n",
      "Epoch [  9  / 50  ]  |  Train Loss:  0.002352  |  Test Loss:  0.002592  |  lr:  0.00100\n",
      "Epoch [ 10  / 50  ]  |  Train Loss:  0.002311  |  Test Loss:  0.002603  |  lr:  0.00100\n",
      "Epoch [ 11  / 50  ]  |  Train Loss:  0.002067  |  Test Loss:  0.002332  |  lr:  0.00010\n",
      "Epoch [ 12  / 50  ]  |  Train Loss:  0.002023  |  Test Loss:  0.002259  |  lr:  0.00010\n",
      "Epoch [ 13  / 50  ]  |  Train Loss:  0.002007  |  Test Loss:  0.002290  |  lr:  0.00010\n",
      "Epoch [ 14  / 50  ]  |  Train Loss:  0.001989  |  Test Loss:  0.002187  |  lr:  0.00010\n",
      "Epoch [ 15  / 50  ]  |  Train Loss:  0.001963  |  Test Loss:  0.002241  |  lr:  0.00010\n",
      "Epoch [ 16  / 50  ]  |  Train Loss:  0.001941  |  Test Loss:  0.002293  |  lr:  0.00010\n",
      "Epoch [ 17  / 50  ]  |  Train Loss:  0.001915  |  Test Loss:  0.002215  |  lr:  0.00010\n",
      "Epoch [ 18  / 50  ]  |  Train Loss:  0.001891  |  Test Loss:  0.002133  |  lr:  0.00010\n",
      "Epoch [ 19  / 50  ]  |  Train Loss:  0.001873  |  Test Loss:  0.002130  |  lr:  0.00010\n",
      "Epoch [ 20  / 50  ]  |  Train Loss:  0.001851  |  Test Loss:  0.002208  |  lr:  0.00010\n",
      "Epoch [ 21  / 50  ]  |  Train Loss:  0.001830  |  Test Loss:  0.002151  |  lr:  0.00010\n",
      "Epoch [ 22  / 50  ]  |  Train Loss:  0.001814  |  Test Loss:  0.002111  |  lr:  0.00010\n",
      "Epoch [ 23  / 50  ]  |  Train Loss:  0.001792  |  Test Loss:  0.002125  |  lr:  0.00010\n",
      "Epoch [ 24  / 50  ]  |  Train Loss:  0.001770  |  Test Loss:  0.002032  |  lr:  0.00010\n",
      "Epoch [ 25  / 50  ]  |  Train Loss:  0.001759  |  Test Loss:  0.002159  |  lr:  0.00010\n",
      "Epoch [ 26  / 50  ]  |  Train Loss:  0.001732  |  Test Loss:  0.002102  |  lr:  0.00010\n",
      "Epoch [ 27  / 50  ]  |  Train Loss:  0.001716  |  Test Loss:  0.002028  |  lr:  0.00010\n",
      "Epoch [ 28  / 50  ]  |  Train Loss:  0.001707  |  Test Loss:  0.001996  |  lr:  0.00010\n",
      "Epoch [ 29  / 50  ]  |  Train Loss:  0.001691  |  Test Loss:  0.001964  |  lr:  0.00010\n",
      "Epoch [ 30  / 50  ]  |  Train Loss:  0.001678  |  Test Loss:  0.002049  |  lr:  0.00010\n",
      "Epoch [ 31  / 50  ]  |  Train Loss:  0.001664  |  Test Loss:  0.002011  |  lr:  0.00010\n",
      "Epoch [ 32  / 50  ]  |  Train Loss:  0.001651  |  Test Loss:  0.002045  |  lr:  0.00010\n",
      "Epoch [ 33  / 50  ]  |  Train Loss:  0.001634  |  Test Loss:  0.002018  |  lr:  0.00010\n",
      "Epoch [ 34  / 50  ]  |  Train Loss:  0.001622  |  Test Loss:  0.002017  |  lr:  0.00010\n",
      "Epoch [ 35  / 50  ]  |  Train Loss:  0.001609  |  Test Loss:  0.002041  |  lr:  0.00010\n",
      "Epoch [ 36  / 50  ]  |  Train Loss:  0.001595  |  Test Loss:  0.002006  |  lr:  0.00010\n",
      "Epoch [ 37  / 50  ]  |  Train Loss:  0.001579  |  Test Loss:  0.001897  |  lr:  0.00010\n",
      "Epoch [ 38  / 50  ]  |  Train Loss:  0.001568  |  Test Loss:  0.001956  |  lr:  0.00010\n",
      "Epoch [ 39  / 50  ]  |  Train Loss:  0.001560  |  Test Loss:  0.001964  |  lr:  0.00010\n",
      "Epoch [ 40  / 50  ]  |  Train Loss:  0.001542  |  Test Loss:  0.001948  |  lr:  0.00010\n",
      "Epoch [ 41  / 50  ]  |  Train Loss:  0.001532  |  Test Loss:  0.001984  |  lr:  0.00010\n",
      "Epoch [ 42  / 50  ]  |  Train Loss:  0.001527  |  Test Loss:  0.001909  |  lr:  0.00010\n",
      "Epoch [ 43  / 50  ]  |  Train Loss:  0.001512  |  Test Loss:  0.001892  |  lr:  0.00010\n",
      "Epoch [ 44  / 50  ]  |  Train Loss:  0.001508  |  Test Loss:  0.001858  |  lr:  0.00010\n",
      "Epoch [ 45  / 50  ]  |  Train Loss:  0.001488  |  Test Loss:  0.001838  |  lr:  0.00010\n",
      "Epoch [ 46  / 50  ]  |  Train Loss:  0.001478  |  Test Loss:  0.001880  |  lr:  0.00010\n",
      "Epoch [ 47  / 50  ]  |  Train Loss:  0.001466  |  Test Loss:  0.001894  |  lr:  0.00010\n",
      "Epoch [ 48  / 50  ]  |  Train Loss:  0.001464  |  Test Loss:  0.001826  |  lr:  0.00010\n",
      "Epoch [ 49  / 50  ]  |  Train Loss:  0.001450  |  Test Loss:  0.001868  |  lr:  0.00010\n",
      "Epoch [ 50  / 50  ]  |  Train Loss:  0.001437  |  Test Loss:  0.001793  |  lr:  0.00010\n"
     ]
    }
   ],
   "source": [
    "name = \"dataSet2_fftSampled_16_3_3e5_model\"\n",
    "folder = f\"models/{name}\"\n",
    "\n",
    "ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_fftSampled_16_3_3e5_series_omega\", transform=transform_2)\n",
    "model_random = train_model(name, ds, model2, folder, load=False,\n",
    "                           scheduler=([10], 0.1),\n",
    "                           epoch=50,\n",
    "                           lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"dataSet2_haltonSampled_6_model\"\n",
    "# folder = f\"models/{name}\"\n",
    "\n",
    "# ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_haltonSampled_6_sdp5_series_omega\", transform=transform_3)\n",
    "# model_halton = train_model(\"dataSet2_haltonSampled_6_model\", ds, model2, folder, load=False,\n",
    "# #                            scheduler=([10], 0.1),\n",
    "#                          epoch=50,\n",
    "#                          lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"dataSet2_sol1_gridSampled_6_model\"\n",
    "folder = f\"models/{name}\"\n",
    "\n",
    "ds = DataSetFSeries.from_parquet(root_dir=\"data\", file_name=\"dataSet2_sol1_gridSampled_6_series_omega\", transform=transform_3)\n",
    "model_sol1 = train_model(\"vdataSet2_sol1_gridSampled_6_model\", ds, model2, folder, load=False,\n",
    "                           scheduler=([10], 0.1),\n",
    "                         epoch=50,\n",
    "                         lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = list(model_halton.children())[1].weight[0]\n",
    "print(len(x_.flatten()))\n",
    "print(list(model_halton.children())[1].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_halton.eval()\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "count = 0\n",
    "for res in list(model_halton.children())[1].weight[::1]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_halton\n",
    "\n",
    "\n",
    "# print(list(model.children())[4].weight.shape)\n",
    "\n",
    "\n",
    "# for res in list(model.children())[4].weight[::2]:\n",
    "# # res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "#     plt.figure(dpi=400, figsize=(10,2))\n",
    "#     plt.axis('off')\n",
    "#     imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(20,1)), cmap='gray')\n",
    "\n",
    "res = list(model.children())[13].weight.detach().cpu().numpy()[:,:,]\n",
    "\n",
    "avg = np.mean(res, axis=1)\n",
    "print(res.shape)\n",
    "\n",
    "plt.figure(dpi=500, figsize=(10,2))\n",
    "plt.axis('off')\n",
    "imgplot = plt.imshow(avg, cmap='gray')\n",
    "# imgplot = plt.imshow(res, cmap='gray')\n",
    "\n",
    "\n",
    "# print(avg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for res in avg:\n",
    "# res = .weight[0]\n",
    "#     res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in list(model.children())[9].weight[::3]:\n",
    "# res = .weight[0]\n",
    "    res2 = res.detach().cpu().numpy().flatten()\n",
    "    plt.figure(dpi=400, figsize=(10,2))\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(np.tile(res2.reshape(1,-1),(1,1)), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigh",
   "language": "python",
   "name": "sigh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
