{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Surrogacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning imports.\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "from collections import OrderedDict\n",
    "from scipy import io\n",
    "\n",
    "# Custom classes.\n",
    "from core import *\n",
    "from dataset import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Visual.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the problem pertains to regression, the optimizer to be used is that of stochastic gradient and descent. The loss function for minimisation is set as the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kristen's datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets provided by Kristen.\n",
    "k_to_do = [KristenDataset(\"dataSet2SplineYawFixedSun.mat\"),\n",
    "           KristenDataset(\"dataSet2SplineYawVaryingSun.mat\"),\n",
    "           KristenDataset(\"datasetSplineYawVaryingSun.mat\"),\n",
    "           KristenDataset(\"datasetSplineYawFixedSun.mat\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save datasets according to test, train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_to_do:\n",
    "    k.save_to_mat()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiated robust scaler from scikit learn.\n",
    "scaler_y1 = preprocessing.RobustScaler()\n",
    "\n",
    "# Apply RobustScaler preprocessing to output variables.\n",
    "for k in k_to_do:\n",
    "#     quantile = np.concatenate(([k.df.loc[:,\"f1\":\"f4\"].quantile(.25).values], [k.df.loc[:,\"f1\":\"f4\"].quantile(.50).values], [k.df.loc[:,\"f1\":\"f4\"].quantile(.75).values]),axis=0).T\n",
    "#     io.savemat(os.path.join(k.filepath,\"quantiles\",f\"{k.filename.split('.')[0]}_quantiles.mat\"), {\"Q1\":np.array([quantile[:,0]]).reshape(-1, 1), \"Q2\":np.array([quantile[:,1]]).reshape(-1, 1),  \"Q3\":np.array([quantile[:,2]]).reshape(-1,1)})\n",
    "\n",
    "#     print(np.array([quantile[:,0]]).reshape(-1, 1))\n",
    "    k.add_f_preprocessing(scaler_y1)\n",
    "    scale = np.array([k._preprocessing_f[0].scale_]).reshape(-1,1)\n",
    "    center = np.array([k._preprocessing_f[0].center_]).reshape(-1,1)\n",
    "    io.savemat(os.path.join(k.filepath,\"post\" , f\"{k.filename.split('.')[0]}_post.mat\"), {\"post_scale\":scale, \"post_center\":center} )\n",
    "#     print(k._preprocessing_f[0].get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 will be used to explore the effect of (1 & 2) modifying the quantity of hidden neurons in each of the two fixed hidden layers and (2) the addition of intermediate batch normalisation after each hidden layers activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dataset.KristenDataset object at 0x7f54dca6a410>\n",
      "models/lr_h200_model_1_kristen_datasetSplineYawVaryingSun\n",
      "(200, 7)\n",
      "(200, 1)\n",
      "(200, 200)\n",
      "(200, 1)\n",
      "(4, 200)\n",
      "(4, 1)\n",
      "lr_h200_model_1_kristen_datasetSplineYawVaryingSun.mat\n",
      "<dataset.KristenDataset object at 0x7f54dcfa2790>\n",
      "models/lr_h200_model_1_kristen_datasetSplineYawFixedSun\n",
      "(200, 7)\n",
      "(200, 1)\n",
      "(200, 200)\n",
      "(200, 1)\n",
      "(4, 200)\n",
      "(4, 1)\n",
      "lr_h200_model_1_kristen_datasetSplineYawFixedSun.mat\n",
      "<dataset.KristenDataset object at 0x7f54dcfa2790>\n",
      "models/lr_h200_model_1_kristen_dataSet2SplineYawVaryingSun\n",
      "(200, 7)\n",
      "(200, 1)\n",
      "(200, 200)\n",
      "(200, 1)\n",
      "(4, 200)\n",
      "(4, 1)\n",
      "lr_h200_model_1_kristen_dataSet2SplineYawVaryingSun.mat\n",
      "<dataset.KristenDataset object at 0x7f54dcfa2790>\n",
      "models/lr_h200_model_1_kristen_dataSet2SplineYawFixedSun\n",
      "(200, 7)\n",
      "(200, 1)\n",
      "(200, 200)\n",
      "(200, 1)\n",
      "(4, 200)\n",
      "(4, 1)\n",
      "lr_h200_model_1_kristen_dataSet2SplineYawFixedSun.mat\n"
     ]
    }
   ],
   "source": [
    "# Define modular structure of model 1.\n",
    "def get_model1_module(hidden_size, data_set, batch_norm=False):\n",
    "    \"\"\"\n",
    "    :param hidden_size: number of neurons for the two hidden layers.\n",
    "    :param data_set: the dataset being used, for defining input and output size.\n",
    "    :param batch_norm: the inclusion of batch normalisation after each hidden layer.\n",
    "    \"\"\"\n",
    "    _sequence = []\n",
    "    print(data_set)\n",
    "    # --- ##  Hidden layer 1\n",
    "    _sequence.append(('fc1', nn.Linear(data_set.input_size, hidden_size)))\n",
    "    # - Sigmoid activation\n",
    "    _sequence.append(('sigmoid1', nn.Sigmoid()))      \n",
    "    # Conditional\n",
    "    if batch_norm:\n",
    "        _sequence.append(('bn1', nn.BatchNorm1d(hidden_size))) \n",
    "    # --- ##  Hidden layer 2\n",
    "    _sequence.append(('fc2', nn.Linear(hidden_size, hidden_size)))      \n",
    "    # - ReLU activation\n",
    "    _sequence.append(('relu2', nn.ReLU()))\n",
    "    # - Conditional \n",
    "    if batch_norm:\n",
    "        _sequence.append(('bn2', nn.BatchNorm1d(hidden_size)))                 \n",
    "    # --- # Output layer\n",
    "    _sequence.append(('output', nn.Linear(hidden_size, data_set.output_size))) \n",
    "    return nn.Sequential(OrderedDict(_sequence))\n",
    "\n",
    "\n",
    "todo = []\n",
    "\n",
    "import os, re\n",
    "for f in os.listdir('models'):\n",
    "    if re.match('lr_h200_model_1_kristen', f):\n",
    "        todo.append(f\"models/{f}\")\n",
    "              \n",
    "def model_checkpoint_to_mat(dataset, model, path):\n",
    "    print(path)\n",
    "    trainer = Trainer(path, dataset, model, torch.optim.SGD, torch.nn.MSELoss, batch_size=100)\n",
    "    trainer.load_checkpoint(path)\n",
    "    model_state = trainer._model.state_dict()\n",
    "#     example_x = dataset.x[0].cuda()\n",
    "#     example_f_real = dataset.f[0].cuda()\n",
    "#     example_f_pred = trainer._model(example_x.cuda().float())\n",
    "\n",
    "#     print(example_x.cpu())\n",
    "#     print(example_f_pred.cpu())\n",
    "#     print(example_f_real.cpu())\n",
    "#     print(dataset._preprocessing_f[0].inverse_transform([example_f_real.cpu().detach().numpy()]))\n",
    "#     print(dataset._preprocessing_f[0].scale_)\n",
    "#     print(dataset._preprocessing_f[0].center_)\n",
    "#     print(model)\n",
    "#     print(dict(model_state))\n",
    "    dict_return = dict()\n",
    "    dict_ = dict(model_state)\n",
    "    for key,value in zip(dict_.keys(), dict_.values()):\n",
    "        arr =  value.cpu().numpy()\n",
    "        if len(arr.shape) == 1:\n",
    "            arr = np.array([arr]).T\n",
    "        print(arr.shape)\n",
    "        dict_return[key.replace(\".\", \"_\")] = value.cpu().numpy()\n",
    "    print(f\"{path.split('/')[1]}.mat\")\n",
    "    io.savemat(f\"data/kristen/models/{path.split('/')[1]}.mat\", dict_return)\n",
    "    \n",
    "dataset = k_to_do[0]\n",
    "# print(k)\n",
    "\n",
    "for dataset, do in zip(k_to_do, todo):\n",
    "    dataset_path = do.split(\"_\")[-1] + \".mat\"\n",
    "    dataset = KristenDataset(dataset_path)\n",
    "    dataset.add_f_preprocessing(scaler_y1)\n",
    "    model_checkpoint_to_mat(dataset, get_model1_module(200, dataset, False), do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Hidden Neurons per layer (h=100), **without** batch normalisation, **without** scheduler\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model1_bn_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3ac46c77ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train models and save.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_to_do\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model1_bn_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1_1_batch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"models/{model1_1_root_name}_{k.filename.split('.')[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model1_bn_module' is not defined"
     ]
    }
   ],
   "source": [
    "# Define quantity of neurons in hidden layers.\n",
    "model1_1_hidden_size = 100\n",
    "model1_1_batch_norm  = False\n",
    "model1_1_root_name = f\"model1_h{model1_1_hidden_size}_lr0.1_bn{model1_1_batch_norm}\"\n",
    "\n",
    "# Train models and save.\n",
    "for idx, k in enumerate(k_to_do):\n",
    "    model_1 = get_model1_bn_module(hidden_size, k, model1_1_batch_norm)\n",
    "    model_folder = f\"models/{model1_1_root_name}_{k.filename.split('.')[0]}\"\n",
    "    trainer = Trainer(model_folder, k, model_1, torch.optim.SGD, torch.nn.MSELoss, num_epochs=150, batch_size=500, learning_rate=1e-1)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Hidden Neurons per layer (h=100), **without** batch normalisation, **with** scheduler\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantity of neurons in hidden layers.\n",
    "model1_2_hidden_size = 100\n",
    "model1_2_batch_norm  = False\n",
    "model1_2_scheduler = True\n",
    "model1_2_root_name = f\"model1_h{model1_2_hidden_size}_lr0.1_bn{model1_2_batch_norm}_scheduler{model1_2_scheduler}\"\n",
    "\n",
    "for idx, k in enumerate(k_to_do):\n",
    "    model_1 = get_model1_bn_module(hidden_size, k)\n",
    "    model_folder = f\"models/{model1_1_root_name}_{k.filename.split('.')[0]}\"\n",
    "    trainer = Trainer(model_folder, k, model_1, torch.optim.SGD, torch.nn.MSELoss, num_epochs=150, batch_size=500, learning_rate=1e-1)\n",
    "    trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=[90,115,140], gamma=0.1)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Hidden Neurons per layer (h=200), **with** batch normalisation, **with scheduler**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_1_hidden_size = 200\n",
    "model1_1_batch_norm  = True\n",
    "\n",
    "def get_model1_bn_module(hidden_size, data_set):\n",
    "    return nn.Sequential(\n",
    "    OrderedDict([\n",
    "                      ('fc1', nn.Linear(data_set.input_size, hidden_size)),\n",
    "                      ('sigmoid1', nn.Sigmoid()),\n",
    "                      ('bn1', nn.BatchNorm1d(hidden_size)),\n",
    "                      ('fc2', nn.Linear(hidden_size, hidden_size)),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('bn2', nn.BatchNorm1d(hidden_size)),\n",
    "                      ('output', nn.Linear(hidden_size, data_set.output_size))\n",
    "                ]))\n",
    "\n",
    "for idx, k in enumerate(k_to_do):\n",
    "    model_1 = get_model1_module(hidden_size, k)\n",
    "    model_folder = f\"models/bn_lr_h200_model_1_kristen_{k.filename.split('.')[0]}\"\n",
    "    trainer = Trainer(model_folder, k, model_1, torch.optim.SGD, torch.nn.MSELoss, num_epochs=150, batch_size=500, learning_rate=1e-1)\n",
    "    trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=[90,115,140], gamma=0.1)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Hidden Neurons per layer (h=200), **with** batch normalisation, **with** scheduler\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantity of neurons in hidden layers.\n",
    "model1_4_hidden_size = 200\n",
    "model1_4_batch_norm  = True\n",
    "model1_4_scheduler = True\n",
    "model1_4_root_name = f\"model1_h{model1_4_hidden_size}_lr0.1_bn{model1_4_batch_norm}_scheduler{model1_4_scheduler}\"\n",
    "\n",
    "for idx, k in enumerate(k_to_do):\n",
    "    model_1 = get_model1_module(model1_4_hidden_size, k, model1_4_batch_norm)\n",
    "    model_folder = f\"models/{model1_4_root_name}_{k.filename.split('.')[0]}\"\n",
    "    trainer = Trainer(model_folder, k, model_1, torch.optim.SGD, torch.nn.MSELoss, num_epochs=120, batch_size=500, learning_rate=1e-1)\n",
    "    trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=[12, 24, 84, 96, 108], gamma=0.1)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dataset.BaseDataset object at 0x7f54dca8f710>\n"
     ]
    }
   ],
   "source": [
    "df_return = None\n",
    "for idx, k in enumerate(k_to_do):\n",
    "    if idx == 0:\n",
    "        f = k.f.cpu().numpy()\n",
    "        x = k.x.cpu().numpy()\n",
    "    else:\n",
    "        f = np.concatenate((f, k.f.cpu().numpy()))\n",
    "        x = np.concatenate((x, k.x.cpu().numpy()))\n",
    "        \n",
    "model1_all_root_name = f\"model1_h{300}_lr0.1_bn{True}_scheduler{True}\"\n",
    "dataset = BaseDataset(f=f, x=x)\n",
    "dataset.filename = \"allDataGiven.mat\"\n",
    "dataset.add_f_preprocessing(scaler_y1)\n",
    "model_all = get_model1_module(300, dataset, True)\n",
    "model_folder = f\"models/{model1_all_root_name}_{dataset.filename.split('.')[0]}\"\n",
    "trainer = Trainer(model_folder, dataset, model_all, torch.optim.SGD, torch.nn.MSELoss, num_epochs=200, batch_size=1000, learning_rate=1e-2)\n",
    "# trainer.add_scheduler(torch.optim.lr_scheduler.MultiStepLR, milestones=[12, 24, 84, 96, 108], gamma=0.1)\n",
    "# trainer.load_checkpoint(model_folder)\n",
    "# trainer.train()\n",
    "scale = np.array([dataset._preprocessing_f[0].scale_]).reshape(-1,1)\n",
    "center = np.array([dataset._preprocessing_f[0].center_]).reshape(-1,1)\n",
    "dataset.sub_dir=\"kristen\"\n",
    "io.savemat(os.path.join(dataset.filepath,\"post\" , f\"{dataset.filename.split('.')[0]}_post.mat\"), {\"post_scale\":scale, \"post_center\":center} )\n",
    "\n",
    "# model_checkpoint_to_mat(dataset, get_model1_module(300, dataset, True), \"models/model1_h300_lr0.1_bnTrue_schedulerTrue_allDataGiven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
