{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "import matplotlib\n",
    "k = KristenDataset(\"dataSet2SplineYawVaryingSun.mat\")\n",
    "g = GeoffreyDataset(\"uniform_12_n_3_omega_0.002.csv\")\n",
    "\n",
    "# %matplotlib inline\n",
    "# matplotlib.style.use('seaborn')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# scatter_matrix(k.df.loc[:,\"f1\":f\"f{k.output_size}\"].tail(30000), figsize=(14,14))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# scaler_y1 = preprocessing.RobustScaler()\n",
    "# # scaler_y2 = preprocessing.MinMaxScaler()\n",
    "# scaler_x = preprocessing.Normalizer()\n",
    "# df_y = scaler_y1.fit_transform(k.f[:2000,:])\n",
    "# # df_y = scaler_y2.fit_transform(df_y)\n",
    "# # df_y = k.f[:2000,:]\n",
    "# df_x = scaler_x.fit_transform(k.x[:2000,:])\n",
    "# # df_x = k.x[:2000,:]\n",
    "\n",
    "# robust_scaled_df = pd.DataFrame(np.concatenate((df_x, df_y),axis=1), columns=k.columns())\n",
    "\n",
    "# # k.df.loc[:,\"f1\":f\"f{k.output_size}\"] = robust_scaled_df.loc[:, \"f1\":f\"f{k.output_size}\"]\n",
    "\n",
    "# scatter_matrix(robust_scaled_df, figsize=(14,14))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset import *\n",
    "k = KristenDataset(\"dataSet2SplineYawFixedSun.mat\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "scaler_y1 = preprocessing.RobustScaler()\n",
    "# scaler_y2 = preprocessing.MinMaxScaler()\n",
    "#caler_x = preprocessing.Normalizer()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import io\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "class DatasetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, y, x):\n",
    "        \n",
    "        #preprocess\n",
    "        self.y = torch.tensor(y)\n",
    "        self.x = torch.tensor(x)\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        return self.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.y.shape[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, we use ToTensor(), so we define the numpy array like (H, W, C)\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def train_test_split(self, frac_test=0.2):\n",
    "        x_train, x_test, f_train, f_test = train_test_split(self.x.cpu().numpy(), self.y.cpu().numpy(), \n",
    "                                                            test_size=frac_test, random_state=42)\n",
    "        \n",
    "        return DatasetTest(x=x_train, y=f_train), DatasetTest(x=x_test, y=f_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyper-parameters\n",
    "# input_size = 7\n",
    "# output_size = 1\n",
    "hidden_size   = 100\n",
    "num_epochs    = 400\n",
    "batch_size    = 200\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "# df_y = torch.tensor(scaler_y1.fit_transform(k.f.cpu().numpy()))\n",
    "# # df_y = df_y[:,:2]\n",
    "# df_x = k.x\n",
    "# tt = DatasetTest(df_y, df_x)\n",
    "\n",
    "\n",
    "# train_, test_ = tt.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Loss and optimizer\n",
    "# # criterion = torch.nn.MSELoss( )\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# from core import *\n",
    "\n",
    "\n",
    "# model_1 = nn.Sequential(OrderedDict([\n",
    "#                       ('fc1', nn.Linear(tt.input_size, hidden_size)),\n",
    "#                       ('sigmoid1', nn.Sigmoid()),\n",
    "#                       ('fc2', nn.Linear(hidden_sizes, hidden_size)),\n",
    "#                       ('relu2', nn.RReLU()),\n",
    "#                       ('output', nn.Linear(hidden_sizes, tt.output_size))\n",
    "# ]))\n",
    "\n",
    "\n",
    "\n",
    "# trainer = Trainer(\"models/model2/\", tt, model_1, torch.optim.SGD, torch.nn.MSELoss)\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# #     def __init__(self, PATH, dataset, model=None, optimizer=None, loss=None, criterion=None, preprocessing=None, batch_size=200, num_epochs=150, learning_rate=1e-2):\n",
    "\n",
    "\n",
    "# model_2 = nn.Sequential(OrderedDict([\n",
    "#                       ('fc1', nn.Linear(tt.input_size, hidden_size)),\n",
    "#                       ('sigmoid1', nn.RReLU()),\n",
    "#                       ('fc2', nn.Linear(hidden_size, hidden_size)),\n",
    "#                       ('relu2', nn.ReLU()),\n",
    "#                       ('output', nn.Linear(hidden_sizes, tt.output_size))\n",
    "# ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "train_, test_ = k.train_test_split(\"test\")\n",
    "\n",
    "print(type(train_[0][0]))\n",
    "print(len(test_))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "# train_loader = k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# # Fully connected neural network with one hidden layer\n",
    "# class NeuralNet(nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)            \n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "# #         self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "# #         self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "#         self.bn1  = nn.BatchNorm1d(hidden_size)\n",
    "#         self.bn2  = nn.BatchNorm1d(hidden_size)\n",
    "#         self.rrelu = nn.RReLU()\n",
    "#         self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.sig(self.fc1(x).float())\n",
    "#         x = self.sig(self.fc2(x).float())\n",
    "# #         x = self.sig(self.fc3(x).float())\n",
    "# #         x = self.sig(self.fc4(x).float())\n",
    "#         x = self.rrelu(self.fc5(x).float())\n",
    "#         return x\n",
    "\n",
    "# model = NeuralNet(tt.input_size, hidden_size, tt.output_size).to(device)\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(k.input_size, hidden_size)),\n",
    "                      ('sigmoid1', nn.Sigmoid()),\n",
    "                      ('fc2', nn.Linear(hidden_size, hidden_size)),\n",
    "                      ('relu2', nn.RReLU()),\n",
    "                      ('output', nn.Linear(hidden_size, k.output_size))\n",
    "])).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss( )\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Epoch [  1  / 400 ];     Train Loss:  4.245806, Test Loss:  2.996585\n",
      "Epoch [  2  / 400 ];     Train Loss:  3.601978, Test Loss:  2.634684\n",
      "Epoch [  3  / 400 ];     Train Loss:  3.124090, Test Loss:  2.282095\n",
      "Epoch [  4  / 400 ];     Train Loss:  2.732642, Test Loss:  2.206914\n",
      "Epoch [  5  / 400 ];     Train Loss:  2.489866, Test Loss:  2.215513\n",
      "Epoch [  6  / 400 ];     Train Loss:  2.288790, Test Loss:  1.758180\n",
      "Epoch [  7  / 400 ];     Train Loss:  2.136725, Test Loss:  2.042856\n",
      "Epoch [  8  / 400 ];     Train Loss:  1.981448, Test Loss:  2.806219\n",
      "Epoch [  9  / 400 ];     Train Loss:  1.869743, Test Loss:  1.917913\n",
      "Epoch [ 10  / 400 ];     Train Loss:  1.781729, Test Loss:  1.371160\n",
      "Epoch [ 11  / 400 ];     Train Loss:  1.713897, Test Loss:  1.354568\n",
      "Epoch [ 12  / 400 ];     Train Loss:  1.643574, Test Loss:  1.952796\n",
      "Epoch [ 13  / 400 ];     Train Loss:  1.581497, Test Loss:  1.274560\n",
      "Epoch [ 14  / 400 ];     Train Loss:  1.545497, Test Loss:  1.216817\n",
      "Epoch [ 15  / 400 ];     Train Loss:  1.492287, Test Loss:  1.314186\n",
      "Epoch [ 16  / 400 ];     Train Loss:  1.441542, Test Loss:  1.503725\n",
      "Epoch [ 17  / 400 ];     Train Loss:  1.414313, Test Loss:  2.432898\n",
      "Epoch [ 18  / 400 ];     Train Loss:  1.382037, Test Loss:  1.133330\n",
      "Epoch [ 19  / 400 ];     Train Loss:  1.347364, Test Loss:  1.365978\n",
      "Epoch [ 20  / 400 ];     Train Loss:  1.311679, Test Loss:  1.028045\n",
      "Epoch [ 21  / 400 ];     Train Loss:  1.301776, Test Loss:  1.009284\n",
      "Epoch [ 22  / 400 ];     Train Loss:  1.277487, Test Loss:  1.058222\n",
      "Epoch [ 23  / 400 ];     Train Loss:  1.247003, Test Loss:  2.806273\n",
      "Epoch [ 24  / 400 ];     Train Loss:  1.229707, Test Loss:  0.916911\n",
      "Epoch [ 25  / 400 ];     Train Loss:  1.197241, Test Loss:  0.940174\n",
      "Epoch [ 26  / 400 ];     Train Loss:  1.185341, Test Loss:  1.000704\n",
      "Epoch [ 27  / 400 ];     Train Loss:  1.168414, Test Loss:  2.418391\n",
      "Epoch [ 28  / 400 ];     Train Loss:  1.150079, Test Loss:  1.222084\n",
      "Epoch [ 29  / 400 ];     Train Loss:  1.119335, Test Loss:  0.859126\n",
      "Epoch [ 30  / 400 ];     Train Loss:  1.107806, Test Loss:  1.530058\n",
      "Epoch [ 31  / 400 ];     Train Loss:  1.087745, Test Loss:  1.449600\n",
      "Epoch [ 32  / 400 ];     Train Loss:  1.073531, Test Loss:  1.285497\n",
      "Epoch [ 33  / 400 ];     Train Loss:  1.053401, Test Loss:  1.028602\n",
      "Epoch [ 34  / 400 ];     Train Loss:  1.039243, Test Loss:  0.887431\n",
      "Epoch [ 35  / 400 ];     Train Loss:  1.028775, Test Loss:  0.868242\n",
      "Epoch [ 36  / 400 ];     Train Loss:  1.010504, Test Loss:  0.757302\n",
      "Epoch [ 37  / 400 ];     Train Loss:  0.992820, Test Loss:  1.018958\n",
      "Epoch [ 38  / 400 ];     Train Loss:  0.981013, Test Loss:  0.774889\n",
      "Epoch [ 39  / 400 ];     Train Loss:  0.961864, Test Loss:  0.855732\n",
      "Epoch [ 40  / 400 ];     Train Loss:  0.951533, Test Loss:  1.265314\n",
      "Epoch [ 41  / 400 ];     Train Loss:  0.943588, Test Loss:  0.737938\n",
      "Epoch [ 42  / 400 ];     Train Loss:  0.916418, Test Loss:  0.787899\n",
      "Epoch [ 43  / 400 ];     Train Loss:  0.908705, Test Loss:  1.194065\n",
      "Epoch [ 44  / 400 ];     Train Loss:  0.900539, Test Loss:  0.712132\n",
      "Epoch [ 45  / 400 ];     Train Loss:  0.887730, Test Loss:  0.692572\n",
      "Epoch [ 46  / 400 ];     Train Loss:  0.884824, Test Loss:  0.719446\n",
      "Epoch [ 47  / 400 ];     Train Loss:  0.867702, Test Loss:  1.094299\n",
      "Epoch [ 48  / 400 ];     Train Loss:  0.856539, Test Loss:  0.664746\n",
      "Epoch [ 49  / 400 ];     Train Loss:  0.851038, Test Loss:  0.935101\n",
      "Epoch [ 50  / 400 ];     Train Loss:  0.841581, Test Loss:  0.750004\n",
      "Epoch [ 51  / 400 ];     Train Loss:  0.835930, Test Loss:  0.687797\n",
      "Epoch [ 52  / 400 ];     Train Loss:  0.826870, Test Loss:  0.651173\n",
      "Epoch [ 53  / 400 ];     Train Loss:  0.811721, Test Loss:  0.690721\n",
      "Epoch [ 54  / 400 ];     Train Loss:  0.811079, Test Loss:  0.709527\n",
      "Epoch [ 55  / 400 ];     Train Loss:  0.804334, Test Loss:  0.782215\n",
      "Epoch [ 56  / 400 ];     Train Loss:  0.798714, Test Loss:  0.705691\n",
      "Epoch [ 57  / 400 ];     Train Loss:  0.791770, Test Loss:  1.144495\n",
      "Epoch [ 58  / 400 ];     Train Loss:  0.788340, Test Loss:  0.727572\n",
      "Epoch [ 59  / 400 ];     Train Loss:  0.779556, Test Loss:  0.675887\n",
      "Epoch [ 60  / 400 ];     Train Loss:  0.773435, Test Loss:  0.669491\n",
      "Epoch [ 61  / 400 ];     Train Loss:  0.767382, Test Loss:  0.949833\n",
      "Epoch [ 62  / 400 ];     Train Loss:  0.763659, Test Loss:  0.673309\n",
      "Epoch [ 63  / 400 ];     Train Loss:  0.756187, Test Loss:  0.637803\n",
      "Epoch [ 64  / 400 ];     Train Loss:  0.750329, Test Loss:  0.636461\n",
      "Epoch [ 65  / 400 ];     Train Loss:  0.746444, Test Loss:  0.785076\n",
      "Epoch [ 66  / 400 ];     Train Loss:  0.744928, Test Loss:  1.054095\n",
      "Epoch [ 67  / 400 ];     Train Loss:  0.742024, Test Loss:  0.704915\n",
      "Epoch [ 68  / 400 ];     Train Loss:  0.728344, Test Loss:  0.616143\n",
      "Epoch [ 69  / 400 ];     Train Loss:  0.724962, Test Loss:  0.922702\n",
      "Epoch [ 70  / 400 ];     Train Loss:  0.721061, Test Loss:  0.582406\n",
      "Epoch [ 71  / 400 ];     Train Loss:  0.724626, Test Loss:  0.602359\n",
      "Epoch [ 72  / 400 ];     Train Loss:  0.709779, Test Loss:  0.603285\n",
      "Epoch [ 73  / 400 ];     Train Loss:  0.708123, Test Loss:  0.572966\n",
      "Epoch [ 74  / 400 ];     Train Loss:  0.708135, Test Loss:  0.711118\n",
      "Epoch [ 75  / 400 ];     Train Loss:  0.696311, Test Loss:  0.718935\n",
      "Epoch [ 76  / 400 ];     Train Loss:  0.694767, Test Loss:  0.565989\n",
      "Epoch [ 77  / 400 ];     Train Loss:  0.690094, Test Loss:  0.592750\n",
      "Epoch [ 78  / 400 ];     Train Loss:  0.687217, Test Loss:  0.986128\n",
      "Epoch [ 79  / 400 ];     Train Loss:  0.684346, Test Loss:  0.557566\n",
      "Epoch [ 80  / 400 ];     Train Loss:  0.679977, Test Loss:  0.580519\n",
      "Epoch [ 81  / 400 ];     Train Loss:  0.684247, Test Loss:  0.762275\n",
      "Epoch [ 82  / 400 ];     Train Loss:  0.676037, Test Loss:  0.556751\n",
      "Epoch [ 83  / 400 ];     Train Loss:  0.671458, Test Loss:  0.548227\n",
      "Epoch [ 84  / 400 ];     Train Loss:  0.671323, Test Loss:  0.558392\n",
      "Epoch [ 85  / 400 ];     Train Loss:  0.666898, Test Loss:  0.548879\n",
      "Epoch [ 86  / 400 ];     Train Loss:  0.655708, Test Loss:  0.549676\n",
      "Epoch [ 87  / 400 ];     Train Loss:  0.663835, Test Loss:  0.544266\n",
      "Epoch [ 88  / 400 ];     Train Loss:  0.658522, Test Loss:  0.600942\n",
      "Epoch [ 89  / 400 ];     Train Loss:  0.652016, Test Loss:  0.549928\n",
      "Epoch [ 90  / 400 ];     Train Loss:  0.648536, Test Loss:  0.589169\n",
      "Epoch [ 91  / 400 ];     Train Loss:  0.652723, Test Loss:  1.054210\n",
      "Epoch [ 92  / 400 ];     Train Loss:  0.644361, Test Loss:  0.661515\n",
      "Epoch [ 93  / 400 ];     Train Loss:  0.646299, Test Loss:  0.529361\n",
      "Epoch [ 94  / 400 ];     Train Loss:  0.634251, Test Loss:  0.544659\n",
      "Epoch [ 95  / 400 ];     Train Loss:  0.636848, Test Loss:  0.964288\n",
      "Epoch [ 96  / 400 ];     Train Loss:  0.638228, Test Loss:  0.581850\n",
      "Epoch [ 97  / 400 ];     Train Loss:  0.630059, Test Loss:  0.519842\n",
      "Epoch [ 98  / 400 ];     Train Loss:  0.628907, Test Loss:  0.732472\n",
      "Epoch [ 99  / 400 ];     Train Loss:  0.628779, Test Loss:  0.535585\n",
      "Epoch [ 100 / 400 ];     Train Loss:  0.621630, Test Loss:  0.575497\n",
      "Epoch [ 101 / 400 ];     Train Loss:  0.621025, Test Loss:  0.756869\n",
      "Epoch [ 102 / 400 ];     Train Loss:  0.623245, Test Loss:  0.518209\n",
      "Epoch [ 103 / 400 ];     Train Loss:  0.618325, Test Loss:  0.535263\n",
      "Epoch [ 104 / 400 ];     Train Loss:  0.612785, Test Loss:  0.503744\n",
      "Epoch [ 105 / 400 ];     Train Loss:  0.605786, Test Loss:  0.518861\n",
      "Epoch [ 106 / 400 ];     Train Loss:  0.606742, Test Loss:  0.498142\n",
      "Epoch [ 107 / 400 ];     Train Loss:  0.610145, Test Loss:  0.528808\n",
      "Epoch [ 108 / 400 ];     Train Loss:  0.605690, Test Loss:  0.521550\n",
      "Epoch [ 109 / 400 ];     Train Loss:  0.597773, Test Loss:  0.939909\n",
      "Epoch [ 110 / 400 ];     Train Loss:  0.597618, Test Loss:  0.502157\n",
      "Epoch [ 111 / 400 ];     Train Loss:  0.593732, Test Loss:  1.424637\n",
      "Epoch [ 112 / 400 ];     Train Loss:  0.594375, Test Loss:  0.549696\n",
      "Epoch [ 113 / 400 ];     Train Loss:  0.591402, Test Loss:  0.509880\n",
      "Epoch [ 114 / 400 ];     Train Loss:  0.587972, Test Loss:  0.585593\n",
      "Epoch [ 115 / 400 ];     Train Loss:  0.591048, Test Loss:  0.494371\n",
      "Epoch [ 116 / 400 ];     Train Loss:  0.577360, Test Loss:  0.636497\n",
      "Epoch [ 117 / 400 ];     Train Loss:  0.583959, Test Loss:  1.110673\n",
      "Epoch [ 118 / 400 ];     Train Loss:  0.574912, Test Loss:  0.538940\n",
      "Epoch [ 119 / 400 ];     Train Loss:  0.577483, Test Loss:  0.487725\n",
      "Epoch [ 120 / 400 ];     Train Loss:  0.575279, Test Loss:  0.614665\n",
      "Epoch [ 121 / 400 ];     Train Loss:  0.571167, Test Loss:  0.547645\n",
      "Epoch [ 122 / 400 ];     Train Loss:  0.571213, Test Loss:  0.543361\n",
      "Epoch [ 123 / 400 ];     Train Loss:  0.567391, Test Loss:  0.493704\n",
      "Epoch [ 124 / 400 ];     Train Loss:  0.568159, Test Loss:  0.550773\n",
      "Epoch [ 125 / 400 ];     Train Loss:  0.560960, Test Loss:  0.480356\n",
      "Epoch [ 126 / 400 ];     Train Loss:  0.562527, Test Loss:  0.472476\n",
      "Epoch [ 127 / 400 ];     Train Loss:  0.563449, Test Loss:  0.511120\n",
      "Epoch [ 128 / 400 ];     Train Loss:  0.558237, Test Loss:  0.654281\n",
      "Epoch [ 129 / 400 ];     Train Loss:  0.555068, Test Loss:  0.477958\n",
      "Epoch [ 130 / 400 ];     Train Loss:  0.557347, Test Loss:  0.684260\n",
      "Epoch [ 131 / 400 ];     Train Loss:  0.549743, Test Loss:  0.525651\n",
      "Epoch [ 132 / 400 ];     Train Loss:  0.551046, Test Loss:  0.765066\n",
      "Epoch [ 133 / 400 ];     Train Loss:  0.556343, Test Loss:  0.460716\n",
      "Epoch [ 134 / 400 ];     Train Loss:  0.542974, Test Loss:  0.535660\n",
      "Epoch [ 135 / 400 ];     Train Loss:  0.545730, Test Loss:  0.471910\n",
      "Epoch [ 136 / 400 ];     Train Loss:  0.541954, Test Loss:  0.564665\n",
      "Epoch [ 137 / 400 ];     Train Loss:  0.539698, Test Loss:  0.454896\n",
      "Epoch [ 138 / 400 ];     Train Loss:  0.545310, Test Loss:  0.463589\n",
      "Epoch [ 139 / 400 ];     Train Loss:  0.537568, Test Loss:  0.457442\n",
      "Epoch [ 140 / 400 ];     Train Loss:  0.539404, Test Loss:  0.531819\n",
      "Epoch [ 141 / 400 ];     Train Loss:  0.534444, Test Loss:  0.452283\n",
      "Epoch [ 142 / 400 ];     Train Loss:  0.531520, Test Loss:  0.540502\n",
      "Epoch [ 143 / 400 ];     Train Loss:  0.534391, Test Loss:  0.477247\n",
      "Epoch [ 144 / 400 ];     Train Loss:  0.535583, Test Loss:  0.466686\n",
      "Epoch [ 145 / 400 ];     Train Loss:  0.527764, Test Loss:  0.451366\n",
      "Epoch [ 146 / 400 ];     Train Loss:  0.528143, Test Loss:  0.466035\n",
      "Epoch [ 147 / 400 ];     Train Loss:  0.521621, Test Loss:  0.459085\n",
      "Epoch [ 148 / 400 ];     Train Loss:  0.523349, Test Loss:  0.619038\n",
      "Epoch [ 149 / 400 ];     Train Loss:  0.523468, Test Loss:  0.726033\n",
      "Epoch [ 150 / 400 ];     Train Loss:  0.519609, Test Loss:  0.487071\n",
      "Epoch [ 151 / 400 ];     Train Loss:  0.520570, Test Loss:  0.450431\n",
      "Epoch [ 152 / 400 ];     Train Loss:  0.517723, Test Loss:  0.452521\n",
      "Epoch [ 153 / 400 ];     Train Loss:  0.514574, Test Loss:  0.460947\n",
      "Epoch [ 154 / 400 ];     Train Loss:  0.519325, Test Loss:  0.440747\n",
      "Epoch [ 155 / 400 ];     Train Loss:  0.508502, Test Loss:  0.474420\n",
      "Epoch [ 156 / 400 ];     Train Loss:  0.511522, Test Loss:  0.431813\n",
      "Epoch [ 157 / 400 ];     Train Loss:  0.511401, Test Loss:  0.495779\n",
      "Epoch [ 158 / 400 ];     Train Loss:  0.509410, Test Loss:  0.472866\n",
      "Epoch [ 159 / 400 ];     Train Loss:  0.507716, Test Loss:  0.701504\n",
      "Epoch [ 160 / 400 ];     Train Loss:  0.505574, Test Loss:  0.601901\n",
      "Epoch [ 161 / 400 ];     Train Loss:  0.505807, Test Loss:  0.501555\n",
      "Epoch [ 162 / 400 ];     Train Loss:  0.505306, Test Loss:  0.455603\n",
      "Epoch [ 163 / 400 ];     Train Loss:  0.500842, Test Loss:  0.639253\n",
      "Epoch [ 164 / 400 ];     Train Loss:  0.501264, Test Loss:  0.449158\n",
      "Epoch [ 165 / 400 ];     Train Loss:  0.503831, Test Loss:  0.442408\n",
      "Epoch [ 166 / 400 ];     Train Loss:  0.501120, Test Loss:  0.496152\n",
      "Epoch [ 167 / 400 ];     Train Loss:  0.497560, Test Loss:  0.433034\n",
      "Epoch [ 168 / 400 ];     Train Loss:  0.498139, Test Loss:  0.631870\n",
      "Epoch [ 169 / 400 ];     Train Loss:  0.494295, Test Loss:  0.422344\n",
      "Epoch [ 170 / 400 ];     Train Loss:  0.492215, Test Loss:  0.573092\n",
      "Epoch [ 171 / 400 ];     Train Loss:  0.495841, Test Loss:  0.432027\n",
      "Epoch [ 172 / 400 ];     Train Loss:  0.491820, Test Loss:  0.444718\n",
      "Epoch [ 173 / 400 ];     Train Loss:  0.492318, Test Loss:  0.500227\n",
      "Epoch [ 174 / 400 ];     Train Loss:  0.487010, Test Loss:  0.422594\n",
      "Epoch [ 175 / 400 ];     Train Loss:  0.490969, Test Loss:  0.420095\n",
      "Epoch [ 176 / 400 ];     Train Loss:  0.487897, Test Loss:  0.421730\n",
      "Epoch [ 177 / 400 ];     Train Loss:  0.491907, Test Loss:  0.656998\n",
      "Epoch [ 178 / 400 ];     Train Loss:  0.484026, Test Loss:  0.427549\n",
      "Epoch [ 179 / 400 ];     Train Loss:  0.485101, Test Loss:  0.429512\n",
      "Epoch [ 180 / 400 ];     Train Loss:  0.486044, Test Loss:  0.438082\n",
      "Epoch [ 181 / 400 ];     Train Loss:  0.485616, Test Loss:  0.422973\n",
      "Epoch [ 182 / 400 ];     Train Loss:  0.480771, Test Loss:  0.651900\n",
      "Epoch [ 183 / 400 ];     Train Loss:  0.484722, Test Loss:  0.411585\n",
      "Epoch [ 184 / 400 ];     Train Loss:  0.479593, Test Loss:  0.413259\n",
      "Epoch [ 185 / 400 ];     Train Loss:  0.479288, Test Loss:  0.498026\n",
      "Epoch [ 186 / 400 ];     Train Loss:  0.478297, Test Loss:  0.411114\n",
      "Epoch [ 187 / 400 ];     Train Loss:  0.477869, Test Loss:  0.436511\n",
      "Epoch [ 188 / 400 ];     Train Loss:  0.479727, Test Loss:  0.736274\n",
      "Epoch [ 189 / 400 ];     Train Loss:  0.480431, Test Loss:  0.735809\n",
      "Epoch [ 190 / 400 ];     Train Loss:  0.473136, Test Loss:  0.730683\n",
      "Epoch [ 191 / 400 ];     Train Loss:  0.478234, Test Loss:  0.409051\n",
      "Epoch [ 192 / 400 ];     Train Loss:  0.472323, Test Loss:  0.437275\n",
      "Epoch [ 193 / 400 ];     Train Loss:  0.471562, Test Loss:  0.430645\n",
      "Epoch [ 194 / 400 ];     Train Loss:  0.473496, Test Loss:  0.660839\n",
      "Epoch [ 195 / 400 ];     Train Loss:  0.473613, Test Loss:  0.569046\n",
      "Epoch [ 196 / 400 ];     Train Loss:  0.471970, Test Loss:  0.541205\n",
      "Epoch [ 197 / 400 ];     Train Loss:  0.468167, Test Loss:  0.677641\n",
      "Epoch [ 198 / 400 ];     Train Loss:  0.468913, Test Loss:  0.433803\n",
      "Epoch [ 199 / 400 ];     Train Loss:  0.475137, Test Loss:  0.423515\n",
      "Epoch [ 200 / 400 ];     Train Loss:  0.467428, Test Loss:  0.438566\n",
      "Epoch [ 201 / 400 ];     Train Loss:  0.467572, Test Loss:  0.423259\n",
      "Epoch [ 202 / 400 ];     Train Loss:  0.469712, Test Loss:  1.056340\n",
      "Epoch [ 203 / 400 ];     Train Loss:  0.468251, Test Loss:  0.403406\n",
      "Epoch [ 204 / 400 ];     Train Loss:  0.464475, Test Loss:  0.458284\n",
      "Epoch [ 205 / 400 ];     Train Loss:  0.464229, Test Loss:  0.446082\n",
      "Epoch [ 206 / 400 ];     Train Loss:  0.467575, Test Loss:  0.397532\n",
      "Epoch [ 207 / 400 ];     Train Loss:  0.466444, Test Loss:  0.399903\n",
      "Epoch [ 208 / 400 ];     Train Loss:  0.464381, Test Loss:  0.398765\n",
      "Epoch [ 209 / 400 ];     Train Loss:  0.462400, Test Loss:  0.452164\n",
      "Epoch [ 210 / 400 ];     Train Loss:  0.460115, Test Loss:  0.407210\n",
      "Epoch [ 211 / 400 ];     Train Loss:  0.464175, Test Loss:  0.476207\n",
      "Epoch [ 212 / 400 ];     Train Loss:  0.464629, Test Loss:  0.437407\n",
      "Epoch [ 213 / 400 ];     Train Loss:  0.459775, Test Loss:  0.453401\n",
      "Epoch [ 214 / 400 ];     Train Loss:  0.457246, Test Loss:  0.410790\n",
      "Epoch [ 215 / 400 ];     Train Loss:  0.456505, Test Loss:  0.415570\n",
      "Epoch [ 216 / 400 ];     Train Loss:  0.458480, Test Loss:  0.948325\n",
      "Epoch [ 217 / 400 ];     Train Loss:  0.456508, Test Loss:  0.478245\n",
      "Epoch [ 218 / 400 ];     Train Loss:  0.456051, Test Loss:  0.396965\n",
      "Epoch [ 219 / 400 ];     Train Loss:  0.453822, Test Loss:  0.412577\n",
      "Epoch [ 220 / 400 ];     Train Loss:  0.457275, Test Loss:  0.507425\n",
      "Epoch [ 221 / 400 ];     Train Loss:  0.455949, Test Loss:  0.393493\n",
      "Epoch [ 222 / 400 ];     Train Loss:  0.452705, Test Loss:  0.410060\n",
      "Epoch [ 223 / 400 ];     Train Loss:  0.451481, Test Loss:  0.392966\n",
      "Epoch [ 224 / 400 ];     Train Loss:  0.452215, Test Loss:  0.468993\n",
      "Epoch [ 225 / 400 ];     Train Loss:  0.455977, Test Loss:  0.399537\n",
      "Epoch [ 226 / 400 ];     Train Loss:  0.451537, Test Loss:  0.414430\n",
      "Epoch [ 227 / 400 ];     Train Loss:  0.448592, Test Loss:  0.429372\n",
      "Epoch [ 228 / 400 ];     Train Loss:  0.450913, Test Loss:  0.468717\n",
      "Epoch [ 229 / 400 ];     Train Loss:  0.450696, Test Loss:  0.446981\n",
      "Epoch [ 230 / 400 ];     Train Loss:  0.448746, Test Loss:  0.570903\n",
      "Epoch [ 231 / 400 ];     Train Loss:  0.449554, Test Loss:  0.516491\n",
      "Epoch [ 232 / 400 ];     Train Loss:  0.447211, Test Loss:  0.419373\n",
      "Epoch [ 233 / 400 ];     Train Loss:  0.445238, Test Loss:  0.473871\n",
      "Epoch [ 234 / 400 ];     Train Loss:  0.442001, Test Loss:  0.576893\n",
      "Epoch [ 235 / 400 ];     Train Loss:  0.450410, Test Loss:  0.449888\n",
      "Epoch [ 236 / 400 ];     Train Loss:  0.447353, Test Loss:  0.408918\n",
      "Epoch [ 237 / 400 ];     Train Loss:  0.445451, Test Loss:  0.389471\n",
      "Epoch [ 238 / 400 ];     Train Loss:  0.443048, Test Loss:  0.405462\n",
      "Epoch [ 239 / 400 ];     Train Loss:  0.440575, Test Loss:  0.547639\n",
      "Epoch [ 240 / 400 ];     Train Loss:  0.439047, Test Loss:  0.484421\n",
      "Epoch [ 241 / 400 ];     Train Loss:  0.445178, Test Loss:  0.489626\n",
      "Epoch [ 242 / 400 ];     Train Loss:  0.441172, Test Loss:  0.407826\n",
      "Epoch [ 243 / 400 ];     Train Loss:  0.440223, Test Loss:  0.556942\n",
      "Epoch [ 244 / 400 ];     Train Loss:  0.441831, Test Loss:  0.434434\n",
      "Epoch [ 245 / 400 ];     Train Loss:  0.441180, Test Loss:  0.471948\n",
      "Epoch [ 246 / 400 ];     Train Loss:  0.440629, Test Loss:  0.381365\n",
      "Epoch [ 247 / 400 ];     Train Loss:  0.437781, Test Loss:  0.518703\n",
      "Epoch [ 248 / 400 ];     Train Loss:  0.435805, Test Loss:  0.415020\n",
      "Epoch [ 249 / 400 ];     Train Loss:  0.437914, Test Loss:  0.376321\n",
      "Epoch [ 250 / 400 ];     Train Loss:  0.437454, Test Loss:  0.387511\n",
      "Epoch [ 251 / 400 ];     Train Loss:  0.437756, Test Loss:  0.391170\n",
      "Epoch [ 252 / 400 ];     Train Loss:  0.435828, Test Loss:  0.383721\n",
      "Epoch [ 253 / 400 ];     Train Loss:  0.437095, Test Loss:  0.378660\n",
      "Epoch [ 254 / 400 ];     Train Loss:  0.434245, Test Loss:  0.442635\n",
      "Epoch [ 255 / 400 ];     Train Loss:  0.439230, Test Loss:  0.394850\n",
      "Epoch [ 256 / 400 ];     Train Loss:  0.432903, Test Loss:  0.534324\n",
      "Epoch [ 257 / 400 ];     Train Loss:  0.433916, Test Loss:  0.435275\n",
      "Epoch [ 258 / 400 ];     Train Loss:  0.436549, Test Loss:  0.407912\n",
      "Epoch [ 259 / 400 ];     Train Loss:  0.433251, Test Loss:  0.373108\n",
      "Epoch [ 260 / 400 ];     Train Loss:  0.431080, Test Loss:  0.372609\n",
      "Epoch [ 261 / 400 ];     Train Loss:  0.433271, Test Loss:  0.377972\n",
      "Epoch [ 262 / 400 ];     Train Loss:  0.431888, Test Loss:  0.862158\n",
      "Epoch [ 263 / 400 ];     Train Loss:  0.431255, Test Loss:  0.420970\n",
      "Epoch [ 264 / 400 ];     Train Loss:  0.429767, Test Loss:  0.459464\n",
      "Epoch [ 265 / 400 ];     Train Loss:  0.429092, Test Loss:  0.371701\n",
      "Epoch [ 266 / 400 ];     Train Loss:  0.427488, Test Loss:  0.367384\n",
      "Epoch [ 267 / 400 ];     Train Loss:  0.427964, Test Loss:  0.432511\n",
      "Epoch [ 268 / 400 ];     Train Loss:  0.429560, Test Loss:  0.370124\n",
      "Epoch [ 269 / 400 ];     Train Loss:  0.430280, Test Loss:  0.616469\n",
      "Epoch [ 270 / 400 ];     Train Loss:  0.427175, Test Loss:  0.380472\n",
      "Epoch [ 271 / 400 ];     Train Loss:  0.427626, Test Loss:  0.380242\n",
      "Epoch [ 272 / 400 ];     Train Loss:  0.425458, Test Loss:  0.371559\n",
      "Epoch [ 273 / 400 ];     Train Loss:  0.423159, Test Loss:  0.364644\n",
      "Epoch [ 274 / 400 ];     Train Loss:  0.427710, Test Loss:  0.440535\n",
      "Epoch [ 275 / 400 ];     Train Loss:  0.425396, Test Loss:  0.369649\n",
      "Epoch [ 276 / 400 ];     Train Loss:  0.426153, Test Loss:  0.369046\n",
      "Epoch [ 277 / 400 ];     Train Loss:  0.423532, Test Loss:  0.388440\n",
      "Epoch [ 278 / 400 ];     Train Loss:  0.422314, Test Loss:  0.373492\n",
      "Epoch [ 279 / 400 ];     Train Loss:  0.427917, Test Loss:  0.376000\n",
      "Epoch [ 280 / 400 ];     Train Loss:  0.417738, Test Loss:  0.362743\n",
      "Epoch [ 281 / 400 ];     Train Loss:  0.422516, Test Loss:  0.543338\n",
      "Epoch [ 282 / 400 ];     Train Loss:  0.420963, Test Loss:  0.498617\n",
      "Epoch [ 283 / 400 ];     Train Loss:  0.425865, Test Loss:  0.523156\n",
      "Epoch [ 284 / 400 ];     Train Loss:  0.423664, Test Loss:  0.378252\n",
      "Epoch [ 285 / 400 ];     Train Loss:  0.418840, Test Loss:  0.363066\n",
      "Epoch [ 286 / 400 ];     Train Loss:  0.422872, Test Loss:  0.378228\n",
      "Epoch [ 287 / 400 ];     Train Loss:  0.420217, Test Loss:  0.644984\n",
      "Epoch [ 288 / 400 ];     Train Loss:  0.420590, Test Loss:  0.405772\n",
      "Epoch [ 289 / 400 ];     Train Loss:  0.416814, Test Loss:  0.364576\n",
      "Epoch [ 290 / 400 ];     Train Loss:  0.416810, Test Loss:  0.362516\n",
      "Epoch [ 291 / 400 ];     Train Loss:  0.419207, Test Loss:  0.359228\n",
      "Epoch [ 292 / 400 ];     Train Loss:  0.416540, Test Loss:  0.518327\n",
      "Epoch [ 293 / 400 ];     Train Loss:  0.417213, Test Loss:  0.362077\n",
      "Epoch [ 294 / 400 ];     Train Loss:  0.415883, Test Loss:  0.438700\n",
      "Epoch [ 295 / 400 ];     Train Loss:  0.416978, Test Loss:  0.359484\n",
      "Epoch [ 296 / 400 ];     Train Loss:  0.416968, Test Loss:  0.362428\n",
      "Epoch [ 297 / 400 ];     Train Loss:  0.417838, Test Loss:  0.371296\n",
      "Epoch [ 298 / 400 ];     Train Loss:  0.418279, Test Loss:  0.532178\n",
      "Epoch [ 299 / 400 ];     Train Loss:  0.412498, Test Loss:  0.360648\n",
      "Epoch [ 300 / 400 ];     Train Loss:  0.410869, Test Loss:  0.538999\n",
      "Epoch [ 301 / 400 ];     Train Loss:  0.413700, Test Loss:  0.513033\n",
      "Epoch [ 302 / 400 ];     Train Loss:  0.416356, Test Loss:  0.646463\n",
      "Epoch [ 303 / 400 ];     Train Loss:  0.412197, Test Loss:  0.458546\n",
      "Epoch [ 304 / 400 ];     Train Loss:  0.413092, Test Loss:  0.358217\n",
      "Epoch [ 305 / 400 ];     Train Loss:  0.411709, Test Loss:  0.478239\n",
      "Epoch [ 306 / 400 ];     Train Loss:  0.413159, Test Loss:  0.375588\n",
      "Epoch [ 307 / 400 ];     Train Loss:  0.413274, Test Loss:  0.440120\n",
      "Epoch [ 308 / 400 ];     Train Loss:  0.410383, Test Loss:  0.361754\n",
      "Epoch [ 309 / 400 ];     Train Loss:  0.411603, Test Loss:  0.361999\n",
      "Epoch [ 310 / 400 ];     Train Loss:  0.409989, Test Loss:  0.352950\n",
      "Epoch [ 311 / 400 ];     Train Loss:  0.412272, Test Loss:  0.435377\n",
      "Epoch [ 312 / 400 ];     Train Loss:  0.417104, Test Loss:  0.353668\n",
      "Epoch [ 313 / 400 ];     Train Loss:  0.407456, Test Loss:  0.535827\n",
      "Epoch [ 314 / 400 ];     Train Loss:  0.409934, Test Loss:  0.353834\n",
      "Epoch [ 315 / 400 ];     Train Loss:  0.411873, Test Loss:  0.366910\n",
      "Epoch [ 316 / 400 ];     Train Loss:  0.406173, Test Loss:  0.358817\n",
      "Epoch [ 317 / 400 ];     Train Loss:  0.411832, Test Loss:  0.357638\n",
      "Epoch [ 318 / 400 ];     Train Loss:  0.405802, Test Loss:  0.379793\n",
      "Epoch [ 319 / 400 ];     Train Loss:  0.409402, Test Loss:  0.372311\n",
      "Epoch [ 320 / 400 ];     Train Loss:  0.408934, Test Loss:  0.354149\n",
      "Epoch [ 321 / 400 ];     Train Loss:  0.403722, Test Loss:  0.371021\n",
      "Epoch [ 322 / 400 ];     Train Loss:  0.409021, Test Loss:  0.350751\n",
      "Epoch [ 323 / 400 ];     Train Loss:  0.407693, Test Loss:  0.412199\n",
      "Epoch [ 324 / 400 ];     Train Loss:  0.402393, Test Loss:  0.382450\n",
      "Epoch [ 325 / 400 ];     Train Loss:  0.405501, Test Loss:  0.362555\n",
      "Epoch [ 326 / 400 ];     Train Loss:  0.407441, Test Loss:  0.399673\n",
      "Epoch [ 327 / 400 ];     Train Loss:  0.407426, Test Loss:  0.496458\n",
      "Epoch [ 328 / 400 ];     Train Loss:  0.401701, Test Loss:  0.351919\n",
      "Epoch [ 329 / 400 ];     Train Loss:  0.404761, Test Loss:  0.364288\n",
      "Epoch [ 330 / 400 ];     Train Loss:  0.406650, Test Loss:  0.356998\n",
      "Epoch [ 331 / 400 ];     Train Loss:  0.405989, Test Loss:  0.358273\n",
      "Epoch [ 332 / 400 ];     Train Loss:  0.401957, Test Loss:  0.368086\n",
      "Epoch [ 333 / 400 ];     Train Loss:  0.403026, Test Loss:  0.404635\n",
      "Epoch [ 334 / 400 ];     Train Loss:  0.400195, Test Loss:  0.350126\n",
      "Epoch [ 335 / 400 ];     Train Loss:  0.408173, Test Loss:  0.360290\n",
      "Epoch [ 336 / 400 ];     Train Loss:  0.400721, Test Loss:  0.388430\n",
      "Epoch [ 337 / 400 ];     Train Loss:  0.404354, Test Loss:  0.345214\n",
      "Epoch [ 338 / 400 ];     Train Loss:  0.401811, Test Loss:  0.827389\n",
      "Epoch [ 339 / 400 ];     Train Loss:  0.401043, Test Loss:  0.451140\n",
      "Epoch [ 340 / 400 ];     Train Loss:  0.402094, Test Loss:  0.368368\n",
      "Epoch [ 341 / 400 ];     Train Loss:  0.400876, Test Loss:  0.347736\n",
      "Epoch [ 342 / 400 ];     Train Loss:  0.400477, Test Loss:  0.378342\n",
      "Epoch [ 343 / 400 ];     Train Loss:  0.400501, Test Loss:  0.353397\n",
      "Epoch [ 344 / 400 ];     Train Loss:  0.401501, Test Loss:  0.345432\n",
      "Epoch [ 345 / 400 ];     Train Loss:  0.398299, Test Loss:  0.676682\n",
      "Epoch [ 346 / 400 ];     Train Loss:  0.400752, Test Loss:  0.354137\n",
      "Epoch [ 347 / 400 ];     Train Loss:  0.393582, Test Loss:  0.346597\n",
      "Epoch [ 348 / 400 ];     Train Loss:  0.398691, Test Loss:  0.386615\n",
      "Epoch [ 349 / 400 ];     Train Loss:  0.395169, Test Loss:  0.380337\n",
      "Epoch [ 350 / 400 ];     Train Loss:  0.400147, Test Loss:  0.342234\n",
      "Epoch [ 351 / 400 ];     Train Loss:  0.397508, Test Loss:  0.351652\n",
      "Epoch [ 352 / 400 ];     Train Loss:  0.397336, Test Loss:  0.560246\n",
      "Epoch [ 353 / 400 ];     Train Loss:  0.397083, Test Loss:  0.471388\n",
      "Epoch [ 354 / 400 ];     Train Loss:  0.398902, Test Loss:  0.353789\n",
      "Epoch [ 355 / 400 ];     Train Loss:  0.393983, Test Loss:  0.374405\n",
      "Epoch [ 356 / 400 ];     Train Loss:  0.397179, Test Loss:  0.399730\n",
      "Epoch [ 357 / 400 ];     Train Loss:  0.394374, Test Loss:  0.343734\n",
      "Epoch [ 358 / 400 ];     Train Loss:  0.397062, Test Loss:  0.469524\n",
      "Epoch [ 359 / 400 ];     Train Loss:  0.393411, Test Loss:  0.400238\n",
      "Epoch [ 360 / 400 ];     Train Loss:  0.395917, Test Loss:  0.382629\n",
      "Epoch [ 361 / 400 ];     Train Loss:  0.396136, Test Loss:  0.520279\n",
      "Epoch [ 362 / 400 ];     Train Loss:  0.393931, Test Loss:  0.339609\n",
      "Epoch [ 363 / 400 ];     Train Loss:  0.393498, Test Loss:  0.407126\n",
      "Epoch [ 364 / 400 ];     Train Loss:  0.392869, Test Loss:  0.487574\n",
      "Epoch [ 365 / 400 ];     Train Loss:  0.393354, Test Loss:  0.375959\n",
      "Epoch [ 366 / 400 ];     Train Loss:  0.394077, Test Loss:  0.344824\n",
      "Epoch [ 367 / 400 ];     Train Loss:  0.391806, Test Loss:  0.346039\n",
      "Epoch [ 368 / 400 ];     Train Loss:  0.393013, Test Loss:  0.437018\n",
      "Epoch [ 369 / 400 ];     Train Loss:  0.392078, Test Loss:  0.367489\n",
      "Epoch [ 370 / 400 ];     Train Loss:  0.394219, Test Loss:  0.339977\n",
      "Epoch [ 371 / 400 ];     Train Loss:  0.389531, Test Loss:  0.402337\n",
      "Epoch [ 372 / 400 ];     Train Loss:  0.393795, Test Loss:  0.399303\n",
      "Epoch [ 373 / 400 ];     Train Loss:  0.391681, Test Loss:  0.412594\n",
      "Epoch [ 374 / 400 ];     Train Loss:  0.388721, Test Loss:  0.515751\n",
      "Epoch [ 375 / 400 ];     Train Loss:  0.392526, Test Loss:  0.400200\n",
      "Epoch [ 376 / 400 ];     Train Loss:  0.390346, Test Loss:  0.338914\n",
      "Epoch [ 377 / 400 ];     Train Loss:  0.389237, Test Loss:  0.335461\n",
      "Epoch [ 378 / 400 ];     Train Loss:  0.393369, Test Loss:  0.342773\n",
      "Epoch [ 379 / 400 ];     Train Loss:  0.385631, Test Loss:  0.438120\n",
      "Epoch [ 380 / 400 ];     Train Loss:  0.392153, Test Loss:  0.359637\n",
      "Epoch [ 381 / 400 ];     Train Loss:  0.387319, Test Loss:  0.378984\n",
      "Epoch [ 382 / 400 ];     Train Loss:  0.389716, Test Loss:  0.462915\n",
      "Epoch [ 383 / 400 ];     Train Loss:  0.386115, Test Loss:  0.357584\n",
      "Epoch [ 384 / 400 ];     Train Loss:  0.389426, Test Loss:  0.400065\n",
      "Epoch [ 385 / 400 ];     Train Loss:  0.390274, Test Loss:  0.333866\n",
      "Epoch [ 386 / 400 ];     Train Loss:  0.387276, Test Loss:  0.364467\n",
      "Epoch [ 387 / 400 ];     Train Loss:  0.384546, Test Loss:  0.477539\n",
      "Epoch [ 388 / 400 ];     Train Loss:  0.390687, Test Loss:  0.340959\n",
      "Epoch [ 389 / 400 ];     Train Loss:  0.386260, Test Loss:  0.370365\n",
      "Epoch [ 390 / 400 ];     Train Loss:  0.388586, Test Loss:  0.522953\n",
      "Epoch [ 391 / 400 ];     Train Loss:  0.388320, Test Loss:  0.365147\n",
      "Epoch [ 392 / 400 ];     Train Loss:  0.387841, Test Loss:  0.742270\n",
      "Epoch [ 393 / 400 ];     Train Loss:  0.386411, Test Loss:  0.333460\n",
      "Epoch [ 394 / 400 ];     Train Loss:  0.384670, Test Loss:  0.498013\n",
      "Epoch [ 395 / 400 ];     Train Loss:  0.384432, Test Loss:  0.372583\n",
      "Epoch [ 396 / 400 ];     Train Loss:  0.386829, Test Loss:  0.334029\n",
      "Epoch [ 397 / 400 ];     Train Loss:  0.384684, Test Loss:  0.335205\n",
      "Epoch [ 398 / 400 ];     Train Loss:  0.384712, Test Loss:  0.331068\n",
      "Epoch [ 399 / 400 ];     Train Loss:  0.384635, Test Loss:  0.381778\n",
      "Epoch [ 400 / 400 ];     Train Loss:  0.384722, Test Loss:  0.509523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model.\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss=0\n",
    "    epoch_test_loss=0\n",
    "    \n",
    "    # Training.\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x.float())\n",
    "        loss = criterion(outputs, y.float())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        epoch_train_loss+=loss.item()\n",
    "        \n",
    "    train_loss_item=epoch_train_loss/len(train_loader)\n",
    "    train_loss.append(train_loss_item)\n",
    "\n",
    "    # Test the model\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x.float())\n",
    "            loss = criterion(outputs, y.float())\n",
    "            epoch_test_loss += loss.item()\n",
    "\n",
    "    test_loss_item=epoch_test_loss/len(test_loader)\n",
    "    test_loss.append(test_loss_item)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1:^5}/{num_epochs:^5}];     Train Loss: {train_loss_item: 8.6f}, Test Loss: {test_loss_item: 8.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2.5), dpi=300)\n",
    "plt.plot(list(range(len(train_loss))), train_loss, color=\"blue\", label=\"Training loss\")\n",
    "plt.plot(list(range(len(test_loss))), test_loss, color=\"orange\", label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"ttest.png\")\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model.ckpt\", map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x.float())\n",
    "        \n",
    "             \n",
    "#         loss = criterion(outputs, y.float())\n",
    "\n",
    "        pred = scaler_y1.inverse_transform(outputs.cpu())\n",
    "        actual=scaler_y1.inverse_transform(y.cpu())\n",
    "        \n",
    "        test = np.array([0.,0.,0.,0.])\n",
    "        while count <= 10:\n",
    "            for i,j in zip(pred, actual):\n",
    "#                 print(np.round((j-i)/i*100,6))\n",
    "#                 print(j)\n",
    "                test+=np.abs(j-i)\n",
    "\n",
    "                print(f\"Pred: {np.round(i,4)}\\nActu: {np.round(j,4)}\\n\")\n",
    "#                 print(f\"Perc: {}\\n\" )\n",
    "            count+=1\n",
    "        print(test/1000)\n",
    "    \n",
    "        if count >= 10:\n",
    "            break\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
